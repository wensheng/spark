"""LLM response caching system.

This module provides a unified caching system for LLM responses across all model providers.
Cache is stored in the user's ~/.cache directory and supports time-based expiration.
"""

import hashlib
import json
import logging
import threading
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Optional

try:
    from filelock import FileLock
except ImportError:
    # Provide a fallback if filelock is not installed
    class FileLock:  # type: ignore
        """Fallback FileLock that does nothing."""

        def __init__(self, lock_file: str, timeout: int = -1):
            pass

        def __enter__(self):
            return self

        def __exit__(self, exc_type, exc_val, exc_tb):
            pass


logger = logging.getLogger(__name__)


@dataclass
class CacheConfig:
    """Configuration for LLM response cache.

    Attributes:
        enabled: Whether caching is enabled
        cache_dir: Directory for cache storage (default: ~/.cache/spark/llm-responses)
        ttl_seconds: Time-to-live for cache entries in seconds (default: 24 hours)
        max_cache_size_mb: Maximum cache size in MB (default: 1GB)
        cleanup_interval_seconds: Interval for automatic cleanup in seconds (default: 1 hour)
    """

    enabled: bool = False
    cache_dir: Optional[Path] = None
    ttl_seconds: int = 86400  # 24 hours
    max_cache_size_mb: int = 1000  # 1GB
    cleanup_interval_seconds: int = 3600  # 1 hour


@dataclass
class CacheEntry:
    """Represents a cached LLM response.

    Attributes:
        cache_key: Unique identifier for this cache entry
        provider: Model provider name (e.g., "openai", "bedrock")
        model_id: Specific model identifier
        cached_at: Timestamp when entry was cached
        expires_at: Timestamp when entry expires
        request: The original request data
        response: The cached response data
        access_count: Number of times this entry has been accessed
        last_accessed: Timestamp of last access
    """

    cache_key: str
    provider: str
    model_id: str
    cached_at: datetime
    expires_at: datetime
    request: dict[str, Any]
    response: dict[str, Any]
    access_count: int = 0
    last_accessed: Optional[datetime] = None


class CacheManager:
    """Manages LLM response caching across all model providers.

    This class provides a singleton instance that handles caching for all model
    providers. It uses a file-based cache stored in the user's ~/.cache directory.

    The cache is thread-safe and supports time-based expiration of entries.
    """

    _instance: Optional["CacheManager"] = None
    _lock = threading.Lock()

    def __init__(self, config: Optional[CacheConfig] = None):
        """Initialize cache manager with configuration.

        Args:
            config: Cache configuration. If None, uses default config.
        """
        self.config = config or CacheConfig()
        self.cache_dir = self.config.cache_dir or Path.home() / ".cache" / "spark" / "llm-responses"
        self._ensure_cache_dir()
        logger.debug(f"Initialized CacheManager with cache_dir={self.cache_dir}")

    @classmethod
    def get_instance(cls, config: Optional[CacheConfig] = None) -> "CacheManager":
        """Get or create singleton cache manager instance.

        Args:
            config: Cache configuration. Only used if instance doesn't exist yet.

        Returns:
            The singleton CacheManager instance.
        """
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = CacheManager(config)
        return cls._instance

    @classmethod
    def reset_instance(cls) -> None:
        """Reset the singleton instance. Useful for testing."""
        with cls._lock:
            cls._instance = None

    def _ensure_cache_dir(self) -> None:
        """Create cache directory structure if it doesn't exist."""
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Ensured cache directory exists: {self.cache_dir}")
        except Exception as e:
            logger.warning(f"Failed to create cache directory: {e}")

    def generate_cache_key(
        self,
        provider: str,
        model_id: str,
        messages: list[dict[str, Any]],
        system_prompt: Optional[str] = None,
        tool_specs: Optional[list[dict[str, Any]]] = None,
        **kwargs: Any,
    ) -> str:
        """Generate deterministic cache key from request parameters.

        The cache key is generated by hashing all parameters that affect the
        model's response. This ensures that identical requests get the same
        cache key while different requests get different keys.

        Args:
            provider: Model provider name (e.g., "openai", "bedrock")
            model_id: Specific model identifier
            messages: The messages array
            system_prompt: Optional system prompt
            tool_specs: Optional tool specifications
            **kwargs: Additional parameters that affect output (temperature, etc.)

        Returns:
            A SHA-256 hash string to use as cache key.
        """
        # Extract only parameters that affect model output
        relevant_params = {
            "temperature": kwargs.get("temperature"),
            "top_p": kwargs.get("top_p"),
            "max_tokens": kwargs.get("max_tokens"),
            "stop_sequences": kwargs.get("stop_sequences"),
            "tool_choice": kwargs.get("tool_choice"),
        }

        # Remove None values
        relevant_params = {k: v for k, v in relevant_params.items() if v is not None}

        # Create deterministic representation
        key_data = {
            "provider": provider,
            "model_id": model_id,
            "messages": messages,
            "system_prompt": system_prompt,
            "tool_specs": tool_specs,
            "params": relevant_params,
        }

        # Serialize to JSON with sorted keys for determinism
        json_str = json.dumps(key_data, sort_keys=True, default=str)

        # Hash to create fixed-length key
        cache_key = hashlib.sha256(json_str.encode()).hexdigest()

        logger.debug(f"Generated cache key: {cache_key[:16]}... for {provider}/{model_id}")
        return cache_key

    def get(self, cache_key: str, provider: str) -> Optional[dict[str, Any]]:
        """Retrieve cached response if exists and not expired.

        Args:
            cache_key: The cache key to look up
            provider: Model provider name

        Returns:
            The cached response dict if found and not expired, None otherwise.
        """
        if not self.config.enabled:
            return None

        cache_file = self._get_cache_file_path(provider, cache_key)

        if not cache_file.exists():
            logger.debug(f"Cache miss: {cache_key[:16]}... (file not found)")
            return None

        # Use file lock to prevent concurrent access issues
        lock_file = cache_file.with_suffix(".lock")
        try:
            with FileLock(str(lock_file), timeout=5):
                try:
                    with open(cache_file, "r") as f:
                        entry_data = json.load(f)

                    entry = self._deserialize_entry(entry_data)

                    # Check expiration
                    if datetime.now() > entry.expires_at:
                        logger.debug(f"Cache expired for key {cache_key[:16]}...")
                        return None

                    # Update access metadata
                    entry.access_count += 1
                    entry.last_accessed = datetime.now()
                    self._save_entry(entry, cache_file)

                    logger.info(f"Cache hit for key {cache_key[:16]}... (access_count={entry.access_count})")
                    return entry.response

                except json.JSONDecodeError as e:
                    logger.warning(f"Invalid JSON in cache file {cache_file}: {e}")
                    return None
                except Exception as e:
                    logger.warning(f"Error reading cache: {e}")
                    return None
        except Exception as e:
            logger.warning(f"Error acquiring lock for cache read: {e}")
            return None

    def set(
        self,
        cache_key: str,
        provider: str,
        model_id: str,
        request: dict[str, Any],
        response: dict[str, Any],
    ) -> None:
        """Store response in cache.

        Args:
            cache_key: The cache key
            provider: Model provider name
            model_id: Specific model identifier
            request: The request data to cache
            response: The response data to cache
        """
        if not self.config.enabled:
            return

        try:
            provider_dir = self.cache_dir / provider
            provider_dir.mkdir(exist_ok=True)

            cache_file = self._get_cache_file_path(provider, cache_key)

            entry = CacheEntry(
                cache_key=cache_key,
                provider=provider,
                model_id=model_id,
                cached_at=datetime.now(),
                expires_at=datetime.now() + timedelta(seconds=self.config.ttl_seconds),
                request=request,
                response=response,
                access_count=0,
            )

            lock_file = cache_file.with_suffix(".lock")
            with FileLock(str(lock_file), timeout=5):
                self._save_entry(entry, cache_file)

            logger.info(
                f"Cached response for key {cache_key[:16]}... "
                f"(provider={provider}, model={model_id}, ttl={self.config.ttl_seconds}s)"
            )
        except Exception as e:
            logger.warning(f"Failed to cache response: {e}")

    def _get_cache_file_path(self, provider: str, cache_key: str) -> Path:
        """Get path to cache file for given key.

        Args:
            provider: Model provider name
            cache_key: The cache key

        Returns:
            Path to the cache file.
        """
        return self.cache_dir / provider / f"{cache_key}.json"

    def _serialize_entry(self, entry: CacheEntry) -> dict[str, Any]:
        """Serialize cache entry to JSON-compatible dict.

        Args:
            entry: The cache entry to serialize

        Returns:
            JSON-compatible dict representation.
        """
        return {
            "version": "1.0",
            "metadata": {
                "provider": entry.provider,
                "model_id": entry.model_id,
                "cached_at": entry.cached_at.isoformat(),
                "expires_at": entry.expires_at.isoformat(),
                "cache_key": entry.cache_key,
                "access_count": entry.access_count,
                "last_accessed": entry.last_accessed.isoformat() if entry.last_accessed else None,
            },
            "request": entry.request,
            "response": entry.response,
        }

    def _deserialize_entry(self, data: dict[str, Any]) -> CacheEntry:
        """Deserialize JSON data to CacheEntry.

        Args:
            data: JSON data to deserialize

        Returns:
            CacheEntry object.
        """
        metadata = data["metadata"]
        return CacheEntry(
            cache_key=metadata["cache_key"],
            provider=metadata["provider"],
            model_id=metadata["model_id"],
            cached_at=datetime.fromisoformat(metadata["cached_at"]),
            expires_at=datetime.fromisoformat(metadata["expires_at"]),
            request=data["request"],
            response=data["response"],
            access_count=metadata.get("access_count", 0),
            last_accessed=datetime.fromisoformat(metadata["last_accessed"]) if metadata.get("last_accessed") else None,
        )

    def _save_entry(self, entry: CacheEntry, cache_file: Path) -> None:
        """Save cache entry to disk.

        Args:
            entry: The cache entry to save
            cache_file: Path to the cache file
        """
        with open(cache_file, "w") as f:
            json.dump(self._serialize_entry(entry), f, indent=2)

    def cleanup_expired(self) -> int:
        """Remove expired cache entries.

        Returns:
            Count of removed entries.
        """
        removed_count = 0
        now = datetime.now()

        try:
            if not self.cache_dir.exists():
                return 0

            for provider_dir in self.cache_dir.iterdir():
                if not provider_dir.is_dir():
                    continue

                for cache_file in provider_dir.glob("*.json"):
                    try:
                        with open(cache_file, "r") as f:
                            entry_data = json.load(f)

                        expires_at = datetime.fromisoformat(entry_data["metadata"]["expires_at"])

                        if now > expires_at:
                            cache_file.unlink()
                            # Also remove lock file if exists
                            lock_file = cache_file.with_suffix(".lock")
                            if lock_file.exists():
                                lock_file.unlink()
                            removed_count += 1
                            logger.debug(f"Removed expired cache file: {cache_file.name}")

                    except Exception as e:
                        logger.warning(f"Error during cleanup of {cache_file}: {e}")

            if removed_count > 0:
                logger.info(f"Cleaned up {removed_count} expired cache entries")

        except Exception as e:
            logger.warning(f"Error during cache cleanup: {e}")

        return removed_count

    def clear_all(self) -> None:
        """Clear all cache entries."""
        import shutil

        try:
            if self.cache_dir.exists():
                shutil.rmtree(self.cache_dir)
                self._ensure_cache_dir()
                logger.info("Cleared all cache entries")
        except Exception as e:
            logger.warning(f"Failed to clear cache: {e}")

    def get_stats(self) -> dict[str, Any]:
        """Get cache statistics.

        Returns:
            Dictionary containing cache statistics including total entries,
            size, and breakdown by provider.
        """
        stats: dict[str, Any] = {"total_entries": 0, "total_size_bytes": 0, "by_provider": {}}

        try:
            if not self.cache_dir.exists():
                return stats

            for provider_dir in self.cache_dir.iterdir():
                if not provider_dir.is_dir():
                    continue

                provider_name = provider_dir.name
                provider_entries = 0
                provider_size = 0

                for cache_file in provider_dir.glob("*.json"):
                    try:
                        provider_entries += 1
                        provider_size += cache_file.stat().st_size
                    except Exception as e:
                        logger.debug(f"Error getting stats for {cache_file}: {e}")

                stats["by_provider"][provider_name] = {"entries": provider_entries, "size_bytes": provider_size}
                stats["total_entries"] += provider_entries
                stats["total_size_bytes"] += provider_size

        except Exception as e:
            logger.warning(f"Error getting cache stats: {e}")

        return stats
