{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Agent Configuration and Memory Management\n",
    "\n",
    "**Difficulty:** Intermediate | **Time:** 30 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Master AgentConfig for structured agent configuration\n",
    "- Understand and implement different memory policies\n",
    "- Configure memory windows and conversation history\n",
    "- Work with preloaded messages for context initialization\n",
    "- Distinguish between agent state and memory\n",
    "- Implement stateless vs stateful agents\n",
    "\n",
    "## Real-World Use Case\n",
    "\n",
    "Imagine you're building AI assistants for different scenarios:\n",
    "\n",
    "- **Customer Support Bot**: Needs to remember the entire conversation to provide context-aware help\n",
    "- **FAQ Assistant**: Stateless, answers each question independently for efficiency\n",
    "- **Therapy Chatbot**: Maintains a rolling window of recent conversation to stay relevant without overwhelming the model\n",
    "- **Code Review Agent**: Preloaded with project context and coding standards\n",
    "- **Research Assistant**: Long-term memory with summarization of previous research sessions\n",
    "\n",
    "Each of these requires different **memory management strategies** and **configuration patterns**. In this tutorial, you'll learn how to configure agents for various memory requirements and understand the tradeoffs between different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### AgentConfig: Centralized Configuration\n",
    "\n",
    "**AgentConfig** is a Pydantic model that centralizes all agent configuration in one place:\n",
    "\n",
    "```python\n",
    "config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-mini'),\n",
    "    system_prompt='You are a helpful assistant.',\n",
    "    prompt_template='User question: {{ query }}',\n",
    "    memory_config=MemoryConfig(policy=MemoryPolicyType.ROLLING_WINDOW, window=10),\n",
    "    preload_messages=[...],\n",
    "    output_mode='text',\n",
    "    tools=[...],\n",
    ")\n",
    "agent = Agent(config)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Type-safe configuration with Pydantic validation\n",
    "- Easy to serialize/deserialize for saving agent configurations\n",
    "- Clear separation between configuration and runtime behavior\n",
    "- Reusable across multiple agent instances\n",
    "\n",
    "### Memory vs. State vs. GraphState: Critical Distinctions\n",
    "\n",
    "**Memory** (Conversation History):\n",
    "- Stores the **message history** sent to the LLM\n",
    "- Managed by `MemoryManager` via `memory_config`\n",
    "- Controls what the LLM \"sees\" from past interactions\n",
    "- Policy-driven (NULL, ROLLING_WINDOW, CUSTOM)\n",
    "- Impacts token usage and context window\n",
    "- **Agent-level scope**\n",
    "\n",
    "**State** (Node State):\n",
    "- Stores **application-level data** like counters, flags, metadata\n",
    "- Node-local, does NOT flow between nodes\n",
    "- Persists across node invocations (e.g., in loops)\n",
    "- Accessed via `context.state`\n",
    "- Does NOT affect LLM input\n",
    "- **Node-level scope**\n",
    "\n",
    "**GraphState** (Global Shared State):\n",
    "- **Shared across all nodes** in a graph\n",
    "- Thread-safe with automatic locking for concurrent execution\n",
    "- Accessed via `context.graph_state`\n",
    "- Enables coordination between multiple agents/nodes\n",
    "- Does NOT affect LLM input (unless explicitly passed)\n",
    "- **Graph-level scope**\n",
    "- Perfect for multi-agent workflows and global tracking\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Memory: What the LLM sees (agent-level)\n",
    "agent.memory = [\n",
    "    {'role': 'user', 'content': 'Hello!'},\n",
    "    {'role': 'assistant', 'content': 'Hi! How can I help?'},\n",
    "]\n",
    "\n",
    "# State: Node-local tracking (NOT sent to LLM)\n",
    "context.state = {\n",
    "    'conversation_id': 'conv_123',\n",
    "    'user_id': 'user_456',\n",
    "    'message_count': 2,\n",
    "}\n",
    "\n",
    "# GraphState: Shared across all nodes (NOT sent to LLM unless passed explicitly)\n",
    "await context.graph_state.set('total_messages', 150)\n",
    "await context.graph_state.set('active_agents', ['agent1', 'agent2'])\n",
    "counter = await context.graph_state.get('workflow_iteration', 0)\n",
    "```\n",
    "\n",
    "### Memory Policies\n",
    "\n",
    "Spark provides three memory policies:\n",
    "\n",
    "**1. NULL Policy** (Stateless)\n",
    "- No conversation history is maintained\n",
    "- Each message is processed independently\n",
    "- Most efficient for token usage\n",
    "- Best for: FAQ bots, simple Q&A, stateless APIs\n",
    "\n",
    "**2. ROLLING_WINDOW Policy** (Bounded Memory)\n",
    "- Keeps only the last N messages\n",
    "- Automatically drops oldest messages when limit is reached\n",
    "- Balances context and token efficiency\n",
    "- Best for: Chat bots, support agents, conversational interfaces\n",
    "\n",
    "**3. CUSTOM Policy** (Advanced)\n",
    "- Use a custom callable to manage memory\n",
    "- Full control over message retention logic\n",
    "- Can implement summarization, importance scoring, etc.\n",
    "- Best for: Complex memory requirements, research assistants, adaptive agents\n",
    "\n",
    "**Note:** The tutorial plan mentioned a SUMMARIZE policy which would automatically summarize old messages to compress context. This is not yet implemented in Spark but can be achieved via CUSTOM policy.\n",
    "\n",
    "### Memory Configuration\n",
    "\n",
    "```python\n",
    "from spark.agents.memory import MemoryConfig, MemoryPolicyType\n",
    "\n",
    "# Stateless agent (no memory)\n",
    "memory_config = MemoryConfig(policy=MemoryPolicyType.NULL)\n",
    "\n",
    "# Rolling window with 10 messages\n",
    "memory_config = MemoryConfig(\n",
    "    policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "    window=10\n",
    ")\n",
    "\n",
    "# Custom memory management\n",
    "def custom_memory_fn(messages: list[dict]) -> list[dict]:\n",
    "    # Keep only high-priority messages\n",
    "    return [m for m in messages if m.get('priority') == 'high']\n",
    "\n",
    "memory_config = MemoryConfig(\n",
    "    policy=MemoryPolicyType.CUSTOM,\n",
    "    callable=custom_memory_fn\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary classes for agent configuration and memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Spark classes\n",
    "from spark.nodes import Node, ExecutionContext, NodeState, default_node_state\n",
    "from spark.agents.agent import Agent\n",
    "from spark.agents.config import AgentConfig\n",
    "from spark.agents.memory import MemoryConfig, MemoryPolicyType\n",
    "from spark.tools import tool\n",
    "from spark.models.openai import OpenAIModel\n",
    "from spark.models.echo import EchoModel\n",
    "from spark.utils import arun\n",
    "from typing import Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic AgentConfig - The Clean Way\n",
    "\n",
    "Let's start by comparing the old way vs. the AgentConfig way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent created with clean, centralized configuration\n",
      "‚úì Agent name: math_tutor\n",
      "‚úì Agent description: A helpful math tutor that explains concepts clearly\n",
      "\n",
      "\n",
      "üìù Agent response: 15 + 27 = 42\n",
      "\n",
      "(10 + 20 = 30, 5 + 7 = 12, and 30 + 12 = 42.)\n"
     ]
    }
   ],
   "source": [
    "config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-mini'),\n",
    "    name='math_tutor',\n",
    "    description='A helpful math tutor that explains concepts clearly',\n",
    "    system_prompt='You are a helpful math tutor.',\n",
    "    prompt_template='Help me solve: {{ problem }}',\n",
    "    output_mode='text',\n",
    ")\n",
    "new_agent = Agent(config)\n",
    "print(\"‚úì Agent created with clean, centralized configuration\")\n",
    "print(f\"‚úì Agent name: {new_agent.config.name}\")\n",
    "print(f\"‚úì Agent description: {new_agent.config.description}\")\n",
    "print()\n",
    "\n",
    "# Test the agent\n",
    "result = await new_agent.run({'problem': 'What is 15 + 27?'})\n",
    "print(f\"\\nüìù Agent response: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding AgentConfig Benefits\n",
    "\n",
    "**Why use AgentConfig?**\n",
    "\n",
    "1. **Centralized Configuration**: All agent settings in one place\n",
    "2. **Type Safety**: Pydantic validation catches errors early\n",
    "3. **Reusability**: Save and load configurations\n",
    "4. **Serialization**: Easy to export to JSON for specs\n",
    "5. **Documentation**: Clear API for what can be configured\n",
    "6. **Testing**: Easy to create test configurations\n",
    "\n",
    "**Key Configuration Options:**\n",
    "- `model`: The LLM model to use\n",
    "- `name`: Identifier for the agent (useful in multi-agent systems)\n",
    "- `description`: What the agent does (for documentation)\n",
    "- `system_prompt`: System instructions for the LLM\n",
    "- `prompt_template`: Jinja2 template for formatting user inputs\n",
    "- `memory_config`: Memory management settings\n",
    "- `preload_messages`: Initial conversation history\n",
    "- `output_mode`: 'text' or 'json' (structured output)\n",
    "- `output_schema`: Pydantic schema for structured output\n",
    "- `tools`: List of tools available to the agent\n",
    "- `callback_handler`: Event handling during execution\n",
    "- `hooks`: Pre/post processing hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Stateless Agent with NULL Memory Policy\n",
    "\n",
    "A stateless agent doesn't remember previous interactions. Each message is processed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 2: Stateless Agent (NULL Memory) ===\n",
      "\n",
      "ü§ñ FAQ Bot (Stateless - No Memory)\n",
      "Memory Policy: null\n",
      "\n",
      "Q1: What are your business hours?\n",
      "A1: What are your business hours?\n",
      "\n",
      "Q2: Do you offer refunds?\n",
      "A2: Do you offer refunds?\n",
      "\n",
      "Q3: What did I just ask you?\n",
      "A3: What did I just ask you?\n",
      "\n",
      "üìä Analysis:\n",
      "‚úì Question 3 asks about previous question, but agent has no memory\n",
      "‚úì Each query is independent - most efficient for simple Q&A\n",
      "‚úì No token overhead from conversation history\n",
      "‚úì Perfect for: FAQ bots, stateless APIs, simple lookups\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Example 2: Stateless Agent (NULL Memory) ===\")\n",
    "print()\n",
    "\n",
    "# Create a stateless FAQ agent\n",
    "faq_config = AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='faq_bot',\n",
    "    system_prompt='You are a FAQ bot. Answer questions based solely on the current question.',\n",
    "    memory_config=MemoryConfig(policy=MemoryPolicyType.NULL),\n",
    ")\n",
    "faq_agent = Agent(faq_config)\n",
    "\n",
    "print(\"ü§ñ FAQ Bot (Stateless - No Memory)\")\n",
    "print(f\"Memory Policy: {faq_agent.config.memory_config.policy}\")\n",
    "print()\n",
    "\n",
    "# Simulate a conversation\n",
    "questions = [\n",
    "    \"What are your business hours?\",\n",
    "    \"Do you offer refunds?\",\n",
    "    \"What did I just ask you?\",  # Agent won't remember!\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"Q{i}: {question}\")\n",
    "    # Use structured content format\n",
    "    result = await faq_agent.do({'messages': [{'role': 'user', 'content': [{'text': question}]}]})\n",
    "    print(f\"A{i}: {result.content}\")\n",
    "    print()\n",
    "\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"‚úì Question 3 asks about previous question, but agent has no memory\")\n",
    "print(\"‚úì Each query is independent - most efficient for simple Q&A\")\n",
    "print(\"‚úì No token overhead from conversation history\")\n",
    "print(\"‚úì Perfect for: FAQ bots, stateless APIs, simple lookups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NULL Memory Policy\n",
    "\n",
    "**When to use NULL policy:**\n",
    "- FAQ bots where context isn't needed\n",
    "- API endpoints that process independent requests\n",
    "- High-volume services where memory overhead is problematic\n",
    "- Classification or single-shot inference tasks\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Most token-efficient (no history sent to LLM)\n",
    "- ‚úÖ Fastest response times\n",
    "- ‚úÖ Stateless and horizontally scalable\n",
    "- ‚ùå No conversation continuity\n",
    "- ‚ùå Cannot reference previous messages\n",
    "- ‚ùå Poor user experience for conversational use cases\n",
    "\n",
    "**Best Practices:**\n",
    "- Use for simple question-answering\n",
    "- Include all necessary context in each request\n",
    "- Set clear expectations with users (\"I don't remember previous messages\")\n",
    "- Consider adding context in system prompt if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Conversational Agent with Rolling Window Memory\n",
    "\n",
    "A rolling window keeps the most recent N messages, providing context without unbounded growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 3: Conversational Agent (Rolling Window Memory) ===\n",
      "\n",
      "ü§ñ Chat Assistant (Rolling Window Memory)\n",
      "Memory Policy: rolling_window\n",
      "Memory Window: 4 messages\n",
      "\n",
      "üí¨ Conversation:\n",
      "\n",
      "Turn 1:\n",
      "  User: Hi! My name is Alice.\n",
      "  Assistant: Hi! My name is Alice.\n",
      "\n",
      "Turn 2:\n",
      "  User: I'm working on a Python project.\n",
      "  Assistant: Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "\n",
      "Turn 3:\n",
      "  User: It's a web application using Flask.\n",
      "  Assistant: Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "\n",
      "Turn 4:\n",
      "  User: What was my name again?\n",
      "  Assistant: I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "  üìù Memory: Should remember Alice (within window)\n",
      "\n",
      "Turn 5:\n",
      "  User: And what framework am I using?\n",
      "  Assistant: It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "And what framework am I using?\n",
      "  üìù Memory: Should remember Flask (within window)\n",
      "\n",
      "Turn 6:\n",
      "  User: What was the first thing I told you?\n",
      "  Assistant: What was my name again?\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "And what framework am I using?\n",
      "It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "Hi! My name is Alice.\n",
      "Hi! My name is Alice.\n",
      "I'm working on a Python project.\n",
      "It's a web application using Flask.\n",
      "What was my name again?\n",
      "And what framework am I using?\n",
      "What was the first thing I told you?\n",
      "  üìù Memory: First message may be outside window now\n",
      "\n",
      "üìä Analysis:\n",
      "‚úì Rolling window maintains recent context\n",
      "‚úì Old messages are automatically dropped\n",
      "‚úì Balances context awareness with token efficiency\n",
      "‚úì Perfect for: Chat bots, support agents, conversational interfaces\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Example 3: Conversational Agent (Rolling Window Memory) ===\")\n",
    "print()\n",
    "\n",
    "# Create an agent with rolling window memory\n",
    "chat_config = AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='chat_assistant',\n",
    "    system_prompt='You are a friendly chat assistant. Remember the conversation context.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=4  # Keep last 4 messages (2 turns)\n",
    "    ),\n",
    ")\n",
    "chat_agent = Agent(chat_config)\n",
    "\n",
    "print(\"ü§ñ Chat Assistant (Rolling Window Memory)\")\n",
    "print(f\"Memory Policy: {chat_agent.config.memory_config.policy}\")\n",
    "print(f\"Memory Window: {chat_agent.config.memory_config.window} messages\")\n",
    "print()\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation = [\n",
    "    \"Hi! My name is Alice.\",\n",
    "    \"I'm working on a Python project.\",\n",
    "    \"It's a web application using Flask.\",\n",
    "    \"What was my name again?\",  # Should remember\n",
    "    \"And what framework am I using?\",  # Should remember\n",
    "    \"What was the first thing I told you?\",  # Might not remember (outside window)\n",
    "]\n",
    "\n",
    "# Note: In the current implementation, we need to manually manage memory\n",
    "# This is a demonstration of how it should work\n",
    "print(\"üí¨ Conversation:\")\n",
    "for i, message in enumerate(conversation, 1):\n",
    "    print(f\"\\nTurn {i}:\")\n",
    "    print(f\"  User: {message}\")\n",
    "    \n",
    "    # Use structured content format\n",
    "    result = await chat_agent.do({'messages': [{'role': 'user', 'content': [{'text': message}]}]})\n",
    "    print(f\"  Assistant: {result.content}\")\n",
    "    \n",
    "    # Demonstrate memory window\n",
    "    if i == 4:\n",
    "        print(\"  üìù Memory: Should remember Alice (within window)\")\n",
    "    elif i == 5:\n",
    "        print(\"  üìù Memory: Should remember Flask (within window)\")\n",
    "    elif i == 6:\n",
    "        print(\"  üìù Memory: First message may be outside window now\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"‚úì Rolling window maintains recent context\")\n",
    "print(\"‚úì Old messages are automatically dropped\")\n",
    "print(\"‚úì Balances context awareness with token efficiency\")\n",
    "print(\"‚úì Perfect for: Chat bots, support agents, conversational interfaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Rolling Window Memory\n",
    "\n",
    "**How it works:**\n",
    "1. New messages are added to memory\n",
    "2. When memory exceeds window size, oldest message is removed (FIFO)\n",
    "3. Only messages in window are sent to LLM\n",
    "\n",
    "**When to use Rolling Window:**\n",
    "- Conversational agents that need recent context\n",
    "- Chat bots with ongoing conversations\n",
    "- Support agents handling customer inquiries\n",
    "- Any scenario where context is needed but not entire history\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Maintains recent conversation context\n",
    "- ‚úÖ Bounded token usage (predictable costs)\n",
    "- ‚úÖ Simple to configure and understand\n",
    "- ‚úÖ Good balance of context and efficiency\n",
    "- ‚ùå Loses older conversation details\n",
    "- ‚ùå May forget important info mentioned early\n",
    "- ‚ö†Ô∏è Need to tune window size for use case\n",
    "\n",
    "**Choosing Window Size:**\n",
    "- **Small (2-4 messages)**: Quick exchanges, simple tasks\n",
    "- **Medium (6-10 messages)**: Standard conversations, most chat bots\n",
    "- **Large (12-20 messages)**: Complex discussions, support sessions\n",
    "- **Very Large (20+ messages)**: Consider CUSTOM policy with summarization\n",
    "\n",
    "**Pro Tip:** Monitor your token usage and adjust window size based on:\n",
    "- Average message length\n",
    "- Model context window limits\n",
    "- Cost constraints\n",
    "- Quality of responses (bigger window = better context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Preloading Messages - Context Initialization\n",
    "\n",
    "Preload messages to initialize an agent with specific context, examples, or conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 4: Preloading Messages ===\n",
      "\n",
      "ü§ñ Code Reviewer (With Preloaded Context)\n",
      "Preloaded messages: 4\n",
      "\n",
      "üìú Preloaded conversation:\n",
      "  User: What is your coding style?\n",
      "  Assistant: I follow PEP 8 for Python, use descriptive variable names, a...\n",
      "  User: What about documentation?\n",
      "  Assistant: I always write docstrings for functions and classes, and inc...\n",
      "\n",
      "üí¨ New conversation:\n",
      "User: Can you review this function? It doesn't have a docstring.\n",
      "Assistant: I can review it, but I don‚Äôt see the function yet. Please paste the function (or a link) and I‚Äôll critique the absence/presence of a docstring and suggest a concrete docstring.\n",
      "\n",
      "In the meantime, here‚Äôs how I‚Äôd review a function that‚Äôs missing a docstring, plus ready-to-use templates you can drop in.\n",
      "\n",
      "Review checklist for a missing docstring\n",
      "- Purpose: Is the function‚Äôs responsibility clear from the code? If not, a brief summary should be included in the docstring.\n",
      "- Summary line: Is there a concise one-line summary at the top?\n",
      "- Parameters (Args/Parameters): Are all parameters documented with a clear, concise description? Are types stated (or compatible with existing type hints)?\n",
      "- Returns: Is the return value documented with its type and meaning?\n",
      "- Raises: Are expected exceptions documented (when input validation fails, or other error conditions arise)?\n",
      "- Side effects and exceptions: Any I/O, network calls, or mutating state should be mentioned if relevant.\n",
      "- Complexity and performance notes: If there are notable time/space complexities, mention them.\n",
      "- Examples: Are usage examples provided to illustrate common cases?\n",
      "- Style consistency: Is the docstring aligned with the project‚Äôs chosen style (Google, NumPy, or reST/Python-style as per PEP 257)?\n",
      "- Language and tone: Clear, precise, and domain-appropriate; avoid ambiguity.\n",
      "- Type hints: If you‚Äôre using type hints, docstring should complement them (not duplicate verbatim).\n",
      "- Keep it up to date: If the function changes, the docstring should be updated accordingly.\n",
      "\n",
      "Docstring templates you can use\n",
      "\n",
      "Option A: Google style\n",
      "def example_function(param1, param2):\n",
      "    \"\"\"\n",
      "    Short summary of what the function does.\n",
      "\n",
      "    Args:\n",
      "        param1 (int): Description of param1.\n",
      "        param2 (str): Description of param2.\n",
      "\n",
      "    Returns:\n",
      "        bool: Description of the return value.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If param1 is negative.\n",
      "        TypeError: If param2 is not a string.\n",
      "    \"\"\"\n",
      "    ...\n",
      "\n",
      "Option B: NumPy style\n",
      "def example_function(param1, param2):\n",
      "    \"\"\"\n",
      "    Short summary of what the function does.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    param1 : int\n",
      "        Description of param1.\n",
      "    param2 : str\n",
      "        Description of param2.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    bool\n",
      "        Description of the return value.\n",
      "\n",
      "    raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If param1 is negative.\n",
      "    TypeError\n",
      "        If param2 is not a string.\n",
      "    \"\"\"\n",
      "    ...\n",
      "\n",
      "Option C: ReST / Sphinx\n",
      "def example_function(param1, param2):\n",
      "    \"\"\"\n",
      "    Short summary of what the function does.\n",
      "\n",
      "    :param param1: Description of param1.\n",
      "    :type param1: int\n",
      "    :param param2: Description of param2.\n",
      "    :type param2: str\n",
      "    :returns: Description of the return value.\n",
      "    :rtype: bool\n",
      "    :raises ValueError: If param1 is negative.\n",
      "    :raises TypeError: If param2 is not a string.\n",
      "    \"\"\"\n",
      "    ...\n",
      "\n",
      "Concrete example (when the function is about computing a discount)\n",
      "def compute_discount(price: float, rate: float) -> float:\n",
      "    \"\"\"\n",
      "    Calculate the discounted price.\n",
      "\n",
      "    Args:\n",
      "        price (float): Original price of the item.\n",
      "        rate (float): Discount rate as a decimal (e.g., 0.2 for 20%).\n",
      "\n",
      "    Returns:\n",
      "        float: The price after applying the discount.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If price is negative or rate is not in [0, 1].\n",
      "    \"\"\"\n",
      "    if price < 0:\n",
      "        raise ValueError(\"price must be non-negative\")\n",
      "    if not (0 <= rate <= 1):\n",
      "        raise ValueError(\"rate must be between 0 and 1\")\n",
      "    return price * (1 - rate)\n",
      "\n",
      "If you want, I can tailor a docstring to your specific function: paste the code (especially the function signature and any relevant behavior), and tell me which docstring style you‚Äôre using (Google, NumPy, or reST). I‚Äôll provide a concrete, idiomatic docstring you can drop in and a quick review note.\n",
      "\n",
      "üìä Analysis:\n",
      "‚úì Agent starts with established context\n",
      "‚úì No need to re-establish standards each session\n",
      "‚úì Saves tokens by not repeating setup information\n",
      "‚úì Perfect for: Domain experts, specialized assistants, code reviewers\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Example 4: Preloading Messages ===\")\n",
    "print()\n",
    "\n",
    "# Create an agent with preloaded conversation\n",
    "preloaded_messages = [\n",
    "    {'role': 'user', 'content': [{'text':'What is your coding style?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text': 'I follow PEP 8 for Python, use descriptive variable names, and prefer functional programming patterns.'}]},\n",
    "    {'role': 'user', 'content': [{'text': 'What about documentation?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text': 'I always write docstrings for functions and classes, and include type hints for clarity.'}]},\n",
    "]\n",
    "\n",
    "code_reviewer_config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='code_reviewer',\n",
    "    system_prompt='You are a code reviewer who has established coding standards.',\n",
    "    preload_messages=preloaded_messages,\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=10\n",
    "    ),\n",
    ")\n",
    "code_reviewer = Agent(code_reviewer_config)\n",
    "\n",
    "print(\"ü§ñ Code Reviewer (With Preloaded Context)\")\n",
    "print(f\"Preloaded messages: {len(preloaded_messages)}\")\n",
    "print()\n",
    "print(\"üìú Preloaded conversation:\")\n",
    "for msg in preloaded_messages:\n",
    "    role = msg['role'].capitalize()\n",
    "    # Extract text from content blocks\n",
    "    text_content = msg['content'][0]['text'] if isinstance(msg['content'], list) else msg['content']\n",
    "    content = text_content[:60] + '...' if len(text_content) > 60 else text_content\n",
    "    print(f\"  {role}: {content}\")\n",
    "print()\n",
    "\n",
    "# Now ask a question that builds on preloaded context\n",
    "print(\"üí¨ New conversation:\")\n",
    "question = \"Can you review this function? It doesn't have a docstring.\"\n",
    "print(f\"User: {question}\")\n",
    "\n",
    "# Use structured content format\n",
    "result = await code_reviewer.do({'messages': [{'role': 'user', 'content': [{'text': question}]}]})\n",
    "print(f\"Assistant: {result.content}\")\n",
    "print()\n",
    "\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"‚úì Agent starts with established context\")\n",
    "print(\"‚úì No need to re-establish standards each session\")\n",
    "print(\"‚úì Saves tokens by not repeating setup information\")\n",
    "print(\"‚úì Perfect for: Domain experts, specialized assistants, code reviewers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Preloaded Messages\n",
    "\n",
    "**Use cases for preloading:**\n",
    "1. **Few-shot examples**: Show the agent how to respond\n",
    "2. **Context initialization**: Establish domain knowledge\n",
    "3. **Persona establishment**: Define agent's character/style\n",
    "4. **Project context**: Load project-specific information\n",
    "5. **Conversation resumption**: Continue a previous session\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Consistent behavior across sessions\n",
    "- ‚úÖ No need to repeat context setup\n",
    "- ‚úÖ Can include few-shot examples\n",
    "- ‚úÖ Works with any memory policy\n",
    "\n",
    "**Best Practices:**\n",
    "- Keep preloaded messages concise (they count toward token limits)\n",
    "- Use for establishing context, not for lengthy histories\n",
    "- Combine with appropriate memory policy\n",
    "- Consider whether preloaded messages should be in the rolling window\n",
    "\n",
    "**Example Use Cases:**\n",
    "```python\n",
    "# Few-shot classification\n",
    "preload_messages = [\n",
    "    {'role': 'user', 'content': 'I love this product!'},\n",
    "    {'role': 'assistant', 'content': 'Sentiment: Positive'},\n",
    "    {'role': 'user', 'content': 'This is terrible.'},\n",
    "    {'role': 'assistant', 'content': 'Sentiment: Negative'},\n",
    "]\n",
    "\n",
    "# Project context\n",
    "preload_messages = [\n",
    "    {'role': 'user', 'content': 'What stack are we using?'},\n",
    "    {'role': 'assistant', 'content': 'Python FastAPI backend, React frontend, PostgreSQL database.'},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Custom Memory Policy - Advanced Control\n",
    "\n",
    "For complex memory requirements, implement a custom memory policy with full control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 5: Custom Memory Policy ===\n",
      "\n",
      "ü§ñ Smart Assistant (Custom Memory Policy)\n",
      "Memory Policy: custom\n",
      "Memory Strategy: Keep all important messages + last 5 regular messages\n",
      "\n",
      "üí¨ Conversation with priority tagging:\n",
      "\n",
      "Simulating memory filtering:\n",
      "  üìä Memory filtered: 8 ‚Üí 7 messages\n",
      "     Important: 2, Recent: 5\n",
      "\n",
      "üìã Retained messages:\n",
      "  ‚≠ê user: Set my budget to $5000\n",
      "  ‚≠ê user: My account number is 12345\n",
      "  üí¨ user: What's the weather?\n",
      "  üí¨ assistant: Sunny today\n",
      "  üí¨ user: Tell me a joke\n",
      "  üí¨ assistant: Why did the chicken...\n",
      "  üí¨ assistant: Account number saved\n",
      "\n",
      "üìä Analysis:\n",
      "‚úì Important information (budget, account) always retained\n",
      "‚úì Recent messages provide context\n",
      "‚úì Old unimportant messages dropped\n",
      "‚úì Perfect for: Long-running agents, research assistants, adaptive memory\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Example 5: Custom Memory Policy ===\")\n",
    "print()\n",
    "\n",
    "# Define a custom memory function\n",
    "def priority_based_memory(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Custom memory policy that keeps:\n",
    "    - All messages marked as 'important'\n",
    "    - Last 5 regular messages\n",
    "    \"\"\"\n",
    "    important_messages = [m for m in messages if m.get('metadata', {}).get('important', False)]\n",
    "    regular_messages = [m for m in messages if not m.get('metadata', {}).get('important', False)]\n",
    "    \n",
    "    # Keep last 5 regular messages\n",
    "    recent_regular = regular_messages[-5:]\n",
    "    \n",
    "    # Combine: important messages + recent regular messages\n",
    "    result = important_messages + recent_regular\n",
    "    \n",
    "    print(f\"  üìä Memory filtered: {len(messages)} ‚Üí {len(result)} messages\")\n",
    "    print(f\"     Important: {len(important_messages)}, Recent: {len(recent_regular)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create agent with custom memory policy\n",
    "custom_config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='smart_assistant',\n",
    "    system_prompt='You are an assistant that prioritizes important information.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.CUSTOM,\n",
    "        callable=priority_based_memory\n",
    "    ),\n",
    ")\n",
    "smart_agent = Agent(custom_config)\n",
    "\n",
    "print(\"ü§ñ Smart Assistant (Custom Memory Policy)\")\n",
    "print(f\"Memory Policy: {smart_agent.config.memory_config.policy}\")\n",
    "print(\"Memory Strategy: Keep all important messages + last 5 regular messages\")\n",
    "print()\n",
    "\n",
    "# Demonstrate custom memory behavior\n",
    "print(\"üí¨ Conversation with priority tagging:\")\n",
    "print()\n",
    "\n",
    "# In a real implementation, you would add messages with metadata\n",
    "# This is a conceptual demonstration\n",
    "messages_demo = [\n",
    "    {'role': 'user', 'content': 'Set my budget to $5000', 'metadata': {'important': True}},\n",
    "    {'role': 'assistant', 'content': 'Budget set to $5000'},\n",
    "    {'role': 'user', 'content': 'What\\'s the weather?'},\n",
    "    {'role': 'assistant', 'content': 'Sunny today'},\n",
    "    {'role': 'user', 'content': 'Tell me a joke'},\n",
    "    {'role': 'assistant', 'content': 'Why did the chicken...'},\n",
    "    {'role': 'user', 'content': 'My account number is 12345', 'metadata': {'important': True}},\n",
    "    {'role': 'assistant', 'content': 'Account number saved'},\n",
    "    # ... many more messages ...\n",
    "]\n",
    "\n",
    "print(\"Simulating memory filtering:\")\n",
    "filtered = priority_based_memory(messages_demo)\n",
    "print()\n",
    "\n",
    "print(\"üìã Retained messages:\")\n",
    "for msg in filtered:\n",
    "    marker = \"‚≠ê\" if msg.get('metadata', {}).get('important') else \"üí¨\"\n",
    "    content = msg['content'][:40] + '...' if len(msg['content']) > 40 else msg['content']\n",
    "    print(f\"  {marker} {msg['role']}: {content}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"‚úì Important information (budget, account) always retained\")\n",
    "print(\"‚úì Recent messages provide context\")\n",
    "print(\"‚úì Old unimportant messages dropped\")\n",
    "print(\"‚úì Perfect for: Long-running agents, research assistants, adaptive memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Custom Memory Policy\n",
    "\n",
    "**When to use CUSTOM policy:**\n",
    "- Complex memory requirements\n",
    "- Importance-based retention\n",
    "- Summarization of old messages\n",
    "- Topic-based memory management\n",
    "- Semantic similarity filtering\n",
    "\n",
    "**Custom Memory Patterns:**\n",
    "\n",
    "**1. Importance-Based (shown above)**\n",
    "- Keep messages marked as important\n",
    "- Drop less relevant messages\n",
    "- Useful for long-running assistants\n",
    "\n",
    "**2. Summarization Pattern**\n",
    "```python\n",
    "def summarizing_memory(messages: list[dict]) -> list[dict]:\n",
    "    if len(messages) > 20:\n",
    "        # Summarize first 10 messages\n",
    "        old_messages = messages[:10]\n",
    "        summary = create_summary(old_messages)  # Use LLM to summarize\n",
    "        recent_messages = messages[10:]\n",
    "        return [{'role': 'system', 'content': f'Summary: {summary}'}] + recent_messages\n",
    "    return messages\n",
    "```\n",
    "\n",
    "**3. Topic-Based Pattern**\n",
    "```python\n",
    "def topic_based_memory(messages: list[dict]) -> list[dict]:\n",
    "    # Keep only messages related to current topic\n",
    "    current_topic = extract_topic(messages[-1])\n",
    "    return [m for m in messages if extract_topic(m) == current_topic]\n",
    "```\n",
    "\n",
    "**4. Semantic Similarity Pattern**\n",
    "```python\n",
    "def semantic_memory(messages: list[dict]) -> list[dict]:\n",
    "    # Keep messages semantically similar to recent queries\n",
    "    recent_query = messages[-1]\n",
    "    embeddings = get_embeddings([m['content'] for m in messages])\n",
    "    similarities = compute_similarity(embeddings[-1], embeddings)\n",
    "    return [m for i, m in enumerate(messages) if similarities[i] > 0.7]\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- Keep custom functions efficient (called on every message)\n",
    "- Always return a valid message list\n",
    "- Consider token limits when retaining messages\n",
    "- Log memory decisions for debugging\n",
    "- Test with realistic conversation lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: State, Memory, and GraphState in Action\n",
    "\n",
    "Let's clearly demonstrate the differences between state, memory, and GraphState with a multi-agent scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Example 6: State, Memory, and GraphState ===\")\n",
    "print()\n",
    "\n",
    "from spark.graphs import Graph\n",
    "\n",
    "class CoordinatorAgent(Agent):\n",
    "    \"\"\"Agent that coordinates workflow and tracks node-level state.\"\"\"\n",
    "    \n",
    "    async def process(self, context: ExecutionContext):\n",
    "        # NODE STATE: Application-level tracking (NOT sent to LLM)\n",
    "        message_count = context.state.get('message_count', 0) + 1\n",
    "        context.state['message_count'] = message_count\n",
    "        context.state['last_timestamp'] = time.time()\n",
    "        \n",
    "        print(f\"üìä Node State (Coordinator - NOT sent to LLM):\")\n",
    "        print(f\"   Message count (this node): {message_count}\")\n",
    "        \n",
    "        # GRAPH STATE: Shared across all nodes\n",
    "        if context.graph_state:\n",
    "            total_messages = await context.graph_state.get('total_workflow_messages', 0)\n",
    "            await context.graph_state.set('total_workflow_messages', total_messages + 1)\n",
    "            \n",
    "            print(f\"\\nüåê Graph State (Shared - NOT sent to LLM):\")\n",
    "            print(f\"   Total workflow messages (all nodes): {total_messages + 1}\")\n",
    "        \n",
    "        # MEMORY: Conversation history (SENT to LLM)\n",
    "        print(f\"\\nüí¨ Memory (Conversation History - SENT to LLM):\")\n",
    "        print(f\"   Managed by MemoryManager, includes all messages\")\n",
    "        print()\n",
    "        \n",
    "        # Call parent process to handle LLM interaction\n",
    "        result = await super().process(context)\n",
    "        return result\n",
    "\n",
    "class ProcessorAgent(Agent):\n",
    "    \"\"\"Another agent to show state isolation and GraphState sharing.\"\"\"\n",
    "    \n",
    "    async def process(self, context: ExecutionContext):\n",
    "        # This agent has its own node state\n",
    "        process_count = context.state.get('process_count', 0) + 1\n",
    "        context.state['process_count'] = process_count\n",
    "        \n",
    "        print(f\"üìä Node State (Processor - NOT sent to LLM):\")\n",
    "        print(f\"   Process count (this node): {process_count}\")\n",
    "        \n",
    "        # But shares GraphState with Coordinator\n",
    "        if context.graph_state:\n",
    "            total_messages = await context.graph_state.get('total_workflow_messages', 0)\n",
    "            await context.graph_state.set('total_workflow_messages', total_messages + 1)\n",
    "            \n",
    "            # Get coordinator's contribution\n",
    "            print(f\"\\nüåê Graph State (Shared - accessible by all nodes):\")\n",
    "            print(f\"   Total workflow messages: {total_messages + 1}\")\n",
    "            print(f\"   (Includes messages from Coordinator agent)\")\n",
    "        \n",
    "        print()\n",
    "        result = await super().process(context)\n",
    "        return result\n",
    "\n",
    "# Create agents with rolling window memory\n",
    "coordinator_state = default_node_state()\n",
    "coordinator_state['message_count'] = 0\n",
    "\n",
    "processor_state = default_node_state()\n",
    "processor_state['process_count'] = 0\n",
    "\n",
    "coordinator_config = AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='coordinator',\n",
    "    system_prompt='You coordinate the workflow.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=3\n",
    "    ),\n",
    "    initial_state=coordinator_state\n",
    ")\n",
    "\n",
    "processor_config = AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='processor',\n",
    "    system_prompt='You process data.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=3\n",
    "    ),\n",
    "    initial_state=processor_state\n",
    ")\n",
    "\n",
    "coordinator = CoordinatorAgent(config=coordinator_config)\n",
    "processor = ProcessorAgent(config=processor_config)\n",
    "\n",
    "# Create a simple graph with both agents\n",
    "coordinator >> processor\n",
    "\n",
    "graph = Graph(\n",
    "    start=coordinator,\n",
    "    initial_state={'total_workflow_messages': 0}  # Initialize GraphState\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Demonstrating State vs Memory vs GraphState\")\n",
    "print()\n",
    "print(\"Scenario: Two agents in a workflow\")\n",
    "print(\"- Coordinator: Tracks its own message_count (node state)\")\n",
    "print(\"- Processor: Tracks its own process_count (node state)\")\n",
    "print(\"- Both: Share total_workflow_messages (graph state)\")\n",
    "print()\n",
    "\n",
    "# Run workflow\n",
    "print(\"--- Running workflow ---\")\n",
    "result = await graph.run({'messages': [{'role': 'user', 'content': [{'text': 'Start workflow'}]}]})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìö Summary: State vs Memory vs GraphState\")\n",
    "print()\n",
    "print(\"NODE STATE (context.state):\")\n",
    "print(\"  ‚úì Node-local tracking (each agent has its own)\")\n",
    "print(\"  ‚úì Does NOT flow between nodes\")\n",
    "print(\"  ‚úì NOT sent to LLM\")\n",
    "print(\"  ‚úì Used for: per-node counters, flags, metadata\")\n",
    "print()\n",
    "print(\"MEMORY (agent.memory / MemoryManager):\")\n",
    "print(\"  ‚úì Conversation history per agent\")\n",
    "print(\"  ‚úì IS sent to LLM as context\")\n",
    "print(\"  ‚úì Managed by memory_config policy\")\n",
    "print(\"  ‚úì Agent-level scope\")\n",
    "print(\"  ‚úì Used for: conversation context and continuity\")\n",
    "print()\n",
    "print(\"GRAPH STATE (context.graph_state):\")\n",
    "print(\"  ‚úì Shared across ALL nodes in the graph\")\n",
    "print(\"  ‚úì Thread-safe with automatic locking\")\n",
    "print(\"  ‚úì NOT sent to LLM (unless explicitly passed)\")\n",
    "print(\"  ‚úì Graph-level scope\")\n",
    "print(\"  ‚úì Used for: multi-agent coordination, global counters, workflow state\")\n",
    "print()\n",
    "print(\"When to use each:\")\n",
    "print(\"  ‚Ä¢ Node State: Track per-agent/per-node metrics\")\n",
    "print(\"  ‚Ä¢ Memory: Enable LLM conversation continuity\")\n",
    "print(\"  ‚Ä¢ Graph State: Coordinate between multiple agents/nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding State, Memory, and GraphState\n",
    "\n",
    "| Aspect | Node State | Memory | GraphState |\n",
    "|--------|-----------|--------|------------|\n",
    "| **Purpose** | Per-node tracking | LLM conversation context | Cross-node coordination |\n",
    "| **Sent to LLM?** | ‚ùå No | ‚úÖ Yes | ‚ùå No (unless passed) |\n",
    "| **Scope** | Node-local | Agent-level | Graph-level (all nodes) |\n",
    "| **Management** | Manual in process() | Automatic via MemoryManager | Automatic with locking |\n",
    "| **Sharing** | Not shared | Not shared | Shared across all nodes |\n",
    "| **Thread-safe** | No (node-local) | No (agent-local) | ‚úÖ Yes (with locks) |\n",
    "| **Token Impact** | None | Direct impact | None (unless passed) |\n",
    "| **Use Cases** | Per-node counters | Conversation continuity | Multi-agent coordination |\n",
    "\n",
    "**When to use NODE STATE:**\n",
    "- Track iteration count in loops\n",
    "- Store user IDs, session IDs\n",
    "- Maintain per-node application flags\n",
    "- Count messages per node\n",
    "- Store data NOT needed by LLM or other nodes\n",
    "\n",
    "**When to use MEMORY:**\n",
    "- Enable conversation continuity\n",
    "- Provide context to LLM\n",
    "- Reference previous messages\n",
    "- Build on earlier discussion\n",
    "- Maintain conversation flow\n",
    "\n",
    "**When to use GRAPH STATE:**\n",
    "- Coordinate between multiple agents in a workflow\n",
    "- Track global counters across all nodes\n",
    "- Share configuration across the graph\n",
    "- Implement multi-agent voting or consensus\n",
    "- Maintain workflow-level state\n",
    "- Aggregate results from multiple agents\n",
    "\n",
    "**GraphState Best Practices:**\n",
    "\n",
    "```python\n",
    "# Initialize graph with initial state\n",
    "graph = Graph(\n",
    "    start=my_node,\n",
    "    initial_state={'counter': 0, 'results': []}\n",
    ")\n",
    "\n",
    "# In node's process() method:\n",
    "async def process(self, context):\n",
    "    # Always check if graph_state is available\n",
    "    if context.graph_state:\n",
    "        # Get value with default\n",
    "        counter = await context.graph_state.get('counter', 0)\n",
    "        \n",
    "        # Update value\n",
    "        await context.graph_state.set('counter', counter + 1)\n",
    "        \n",
    "        # Batch update\n",
    "        await context.graph_state.update({'x': 1, 'y': 2})\n",
    "        \n",
    "        # Atomic transactions for read-modify-write\n",
    "        async with context.graph_state.transaction() as state:\n",
    "            state['x'] = state.get('x', 0) + 1\n",
    "            state['y'] = state.get('y', 0) + 1\n",
    "    \n",
    "    return {'done': True}\n",
    "\n",
    "# After graph execution, access final state\n",
    "result = await graph.run()\n",
    "final_counter = await graph.get_state('counter')\n",
    "snapshot = graph.get_state_snapshot()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 7: Complete AgentConfig ===\n",
      "\n",
      "ü§ñ Advanced Assistant (Full Configuration)\n",
      "\n",
      "üìã Configuration Summary:\n",
      "  Name: advanced_assistant\n",
      "  Description: A fully configured assistant with all features\n",
      "  Memory Policy: rolling_window\n",
      "  Memory Window: 8\n",
      "  Preloaded Messages: 2\n",
      "  Tools: 1\n",
      "  Hooks: 1 before, 2 after\n",
      "  Output Mode: text\n",
      "\n",
      "üí¨ Testing agent:\n",
      "User: What time is it?\n",
      "\n",
      "before_call_hook payload: [{'content': [{'text': 'What can you help me with?'}], 'role': 'user'}, {'content': [{'text': 'I can help with questions, use tools, and maintain conversation context.'}], 'role': 'assistant'}, {'role': 'user', 'content': [{'text': 'User: What time is it?\\nContext: Testing full configuration'}]}]\n",
      "  üîî Hook: About to call LLM\n",
      "tool_call: {'id': 'call_wLIoVeGuN8bBceP9jpJsH0rp', 'type': 'function', 'function': {'name': 'get_current_time', 'arguments': {}}}\n",
      "after_call_hook payload: id=None type='text' content='Current time: 15:54:14 (server time). Would you like me to convert to another time zone or set a reminder?' raw=[] metadata={} extras={}\n",
      "  üîî Hook: LLM call completed\n",
      "\n",
      "Assistant: Current time: 15:54:14 (server time). Would you like me to convert to another time zone or set a reminder?\n",
      "\n",
      "üìä Analysis:\n",
      "‚úì All configuration options demonstrated\n",
      "‚úì Hooks called before and after LLM\n",
      "‚úì Tools available for use\n",
      "‚úì Memory policy active\n",
      "‚úì Initial state set\n",
      "‚úì Prompt template applied\n",
      "‚úì Ready for production use\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Example 7: Complete AgentConfig ===\")\n",
    "print()\n",
    "\n",
    "# Define a simple tool\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time.\"\"\"\n",
    "    return time.strftime('%H:%M:%S')\n",
    "\n",
    "# Define hooks for monitoring\n",
    "async def before_call_hook(msgs, context) -> None:\n",
    "    print('before_call_hook payload:', msgs)\n",
    "    print(\"  üîî Hook: About to call LLM\")\n",
    "\n",
    "async def after_call_hook(result, context) -> None:\n",
    "    print('after_call_hook payload:', result)\n",
    "    print(\"  üîî Hook: LLM call completed\")\n",
    "\n",
    "init_state = default_node_state()\n",
    "init_state.update({'session_id': 'session_123', 'user_id': 'user_456'})\n",
    "# Create comprehensive configuration\n",
    "full_config = AgentConfig(\n",
    "    # Core settings\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='advanced_assistant',\n",
    "    description='A fully configured assistant with all features',\n",
    "    \n",
    "    # Prompting\n",
    "    system_prompt='You are an advanced AI assistant with access to tools and context.',\n",
    "    prompt_template='User: {{ query }}\\nContext: {{ context }}',\n",
    "    \n",
    "    # Memory management\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=8\n",
    "    ),\n",
    "    \n",
    "    # Context initialization\n",
    "    preload_messages=[\n",
    "        {'role': 'user', 'content': [{'text':'What can you help me with?'}]},\n",
    "        {'role': 'assistant', 'content': [{'text':'I can help with questions, use tools, and maintain conversation context.'}]},\n",
    "    ],\n",
    "    \n",
    "    # Output configuration\n",
    "    output_mode='text',\n",
    "    \n",
    "    # Tools\n",
    "    tools=[get_current_time],\n",
    "    \n",
    "    # Hooks and callbacks\n",
    "    before_llm_hooks=[before_call_hook],\n",
    "    after_llm_hooks=[after_call_hook],\n",
    "\n",
    "    # Initial state\n",
    "    initial_state=init_state,\n",
    ")\n",
    "\n",
    "# Create agent\n",
    "advanced_agent = Agent(full_config)\n",
    "\n",
    "print(\"ü§ñ Advanced Assistant (Full Configuration)\")\n",
    "print()\n",
    "print(\"üìã Configuration Summary:\")\n",
    "print(f\"  Name: {advanced_agent.config.name}\")\n",
    "print(f\"  Description: {advanced_agent.config.description}\")\n",
    "print(f\"  Memory Policy: {advanced_agent.config.memory_config.policy}\")\n",
    "print(f\"  Memory Window: {advanced_agent.config.memory_config.window}\")\n",
    "print(f\"  Preloaded Messages: {len(advanced_agent.config.preload_messages)}\")\n",
    "print(f\"  Tools: {len(advanced_agent.config.tools)}\")\n",
    "print(f\"  Hooks: {len(advanced_agent.config.before_llm_hooks)} before, {len(advanced_agent.config.after_llm_hooks)} after\")\n",
    "print(f\"  Output Mode: {advanced_agent.config.output_mode}\")\n",
    "print()\n",
    "\n",
    "# Test the agent\n",
    "print(\"üí¨ Testing agent:\")\n",
    "query = \"What time is it?\"\n",
    "print(f\"User: {query}\")\n",
    "print()\n",
    "\n",
    "result = await advanced_agent.do({\n",
    "    'query': query,\n",
    "    'context': 'Testing full configuration'\n",
    "})\n",
    "\n",
    "print()\n",
    "print(f\"Assistant: {result.content}\")\n",
    "print()\n",
    "\n",
    "print(\"üìä Analysis:\")\n",
    "print(\"‚úì All configuration options demonstrated\")\n",
    "print(\"‚úì Hooks called before and after LLM\")\n",
    "print(\"‚úì Tools available for use\")\n",
    "print(\"‚úì Memory policy active\")\n",
    "print(\"‚úì Initial state set\")\n",
    "print(\"‚úì Prompt template applied\")\n",
    "print(\"‚úì Ready for production use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí™ Hands-On Exercises\n",
    "\n",
    "Test your understanding with these practical exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Recap\n",
    "\n",
    "### AgentConfig: Your Configuration Hub\n",
    "\n",
    "```python\n",
    "config = AgentConfig(\n",
    "    model=...,              # Required: LLM model\n",
    "    name=...,               # Agent identifier\n",
    "    description=...,        # What the agent does\n",
    "    system_prompt=...,      # System instructions\n",
    "    prompt_template=...,    # Jinja2 formatting\n",
    "    memory_config=...,      # Memory management\n",
    "    preload_messages=...,   # Initial context\n",
    "    output_mode=...,        # 'text' or 'json'\n",
    "    output_schema=...,      # Pydantic schema\n",
    "    tools=...,              # Available tools\n",
    "    before_llm_hooks=...,   # Pre-LLM hooks\n",
    "    after_llm_hooks=...,    # Post-LLM hooks\n",
    "    initial_state=...,      # Starting node state\n",
    ")\n",
    "```\n",
    "\n",
    "### Memory Policies Comparison\n",
    "\n",
    "| Policy | Best For | Token Impact | Context Retention |\n",
    "|--------|----------|--------------|-------------------|\n",
    "| **NULL** | FAQ bots, stateless APIs | Minimal | None |\n",
    "| **ROLLING_WINDOW** | Chat bots, support agents | Bounded | Recent messages |\n",
    "| **CUSTOM** | Complex requirements | Variable | Fully customizable |\n",
    "\n",
    "### State vs Memory vs GraphState (Quick Reference)\n",
    "\n",
    "```python\n",
    "# NODE STATE: Per-node tracking (NOT sent to LLM)\n",
    "context.state['message_count'] = 5\n",
    "context.state['user_id'] = 'user_123'\n",
    "\n",
    "# MEMORY: Conversation history (SENT to LLM)\n",
    "# Managed automatically by MemoryManager\n",
    "agent.config.memory_config = MemoryConfig(\n",
    "    policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "    window=10\n",
    ")\n",
    "\n",
    "# GRAPH STATE: Shared across all nodes (NOT sent to LLM)\n",
    "# Available when agent is part of a graph\n",
    "if context.graph_state:\n",
    "    await context.graph_state.set('total_messages', 100)\n",
    "    counter = await context.graph_state.get('workflow_step', 0)\n",
    "```\n",
    "\n",
    "### When to Use Each Memory Policy\n",
    "\n",
    "**Use NULL when:**\n",
    "- Processing independent requests\n",
    "- Building stateless APIs\n",
    "- Token efficiency is critical\n",
    "- No conversation continuity needed\n",
    "\n",
    "**Use ROLLING_WINDOW when:**\n",
    "- Building conversational agents\n",
    "- Need recent context\n",
    "- Want predictable token usage\n",
    "- Typical chat bot scenarios\n",
    "\n",
    "**Use CUSTOM when:**\n",
    "- Need importance-based retention\n",
    "- Implementing summarization\n",
    "- Topic-based memory filtering\n",
    "- Complex memory requirements\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "1. **Always use AgentConfig** for new agents (cleaner, more maintainable)\n",
    "2. **Choose memory policy** based on use case, not default\n",
    "3. **Monitor token usage** and adjust window size accordingly\n",
    "4. **Use state for tracking**, memory for conversation, **GraphState for coordination**\n",
    "5. **Preload important context** to save tokens and setup time\n",
    "6. **Test memory behavior** with realistic conversation lengths\n",
    "7. **Document memory decisions** in your code\n",
    "8. **Consider custom policies** for advanced use cases\n",
    "9. **Use GraphState** when agents need to coordinate in multi-agent workflows\n",
    "10. **Always check** if `context.graph_state` is available before using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# TODO: Define company policy preload messages\n",
    "# TODO: Create AgentConfig with rolling window (6 messages)\n",
    "# TODO: Create agent that tracks question count in state\n",
    "# TODO: Test with a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Stateless Q&A Agent\n",
    "\n",
    "Build a FAQ agent that:\n",
    "- Uses NULL memory policy\n",
    "- Has a clear system prompt about being stateless\n",
    "- Includes preloaded FAQ examples\n",
    "- Processes each question independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# TODO: Create AgentConfig with NULL policy\n",
    "# TODO: Preload FAQ examples\n",
    "# TODO: Test with follow-up questions to verify statelessness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement a Custom Memory Policy\n",
    "\n",
    "Create a research assistant with custom memory that:\n",
    "- Keeps all messages tagged with 'research' metadata\n",
    "- Keeps the last 3 regular messages\n",
    "- Drops all other messages\n",
    "- Test with a mix of research and casual messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# TODO: Define custom memory function\n",
    "# TODO: Create AgentConfig with CUSTOM policy\n",
    "# TODO: Test with tagged messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Memory Policy Comparison\n",
    "\n",
    "Create three versions of the same agent with different memory policies:\n",
    "- Version A: NULL policy\n",
    "- Version B: ROLLING_WINDOW with window=4\n",
    "- Version C: ROLLING_WINDOW with window=10\n",
    "\n",
    "Run the same conversation through all three and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# TODO: Create three agent configurations\n",
    "# TODO: Define a test conversation\n",
    "# TODO: Run conversation through all three agents\n",
    "# TODO: Compare and analyze the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Solutions\n",
    "\n",
    "Try the exercises yourself first! Solutions are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Solution 1: Limited-Memory Support Agent ===\n",
      "\n",
      "User: Can I return a product I bought 20 days ago?\n",
      "üìä Question #1\n",
      "Assistant: Yes. You‚Äôre within our 30-day return window as long as you have the receipt. If you‚Äôd like, I can help start the return‚Äîcould you share your order number or the email used for the purchase? Also tell me whether you‚Äôd prefer a refund or an exchange.\n",
      "\n",
      "User: Are you open on weekends?\n",
      "üìä Question #2\n",
      "########################################################\n",
      "error: content_type=<Y> | unsupported type\n",
      "########################################################\n",
      "Assistant: {'error': 'content_type=<Y> | unsupported type'}\n",
      "\n",
      "User: What about the return policy again?\n",
      "üìä Question #3\n",
      "########################################################\n",
      "error: content_type=<Y> | unsupported type\n",
      "########################################################\n",
      "Assistant: {'error': 'content_type=<Y> | unsupported type'}\n",
      "\n",
      "‚úÖ Total questions asked: 3\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Limited-Memory Support Agent\n",
    "\n",
    "# Company policies to preload\n",
    "company_policies = [\n",
    "    {'role': 'user', 'content': [{'text':'What is your return policy?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'We offer 30-day returns for all products with receipt.'}]},\n",
    "    {'role': 'user', 'content': [{'text':'What are your business hours?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'We are open Monday-Friday 9am-6pm EST.'}]},\n",
    "]\n",
    "\n",
    "class SupportAgent(Agent):\n",
    "    \"\"\"Support agent that tracks questions.\"\"\"\n",
    "    \n",
    "    async def process(self, context: ExecutionContext):\n",
    "        # Track question count in state\n",
    "        question_count = context.state.get('question_count', 0) + 1\n",
    "        context.state['question_count'] = question_count\n",
    "        \n",
    "        print(f\"üìä Question #{question_count}\")\n",
    "        \n",
    "        result = await super().process(context)\n",
    "        return result\n",
    "\n",
    "# Create configuration\n",
    "support_config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='support_agent',\n",
    "    system_prompt='You are a friendly customer support agent. Help users with their questions about our products and policies.',\n",
    "    preload_messages=company_policies,\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=6\n",
    "    ),\n",
    "    initial_state=default_node_state(question_count=0)\n",
    ")\n",
    "\n",
    "support_agent = SupportAgent(config=support_config)\n",
    "\n",
    "print(\"=== Solution 1: Limited-Memory Support Agent ===\")\n",
    "print()\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Can I return a product I bought 20 days ago?\",\n",
    "    \"Are you open on weekends?\",\n",
    "    \"What about the return policy again?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"User: {q}\")\n",
    "    # Use structured content format to match preloaded messages\n",
    "    result = await support_agent.do({'messages': [{'role': 'user', 'content': [{'text': q}]}]})\n",
    "    print(f\"Assistant: {result.content}\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Total questions asked: {support_agent._state['question_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Solution 1: Limited-Memory Support Agent ===\n",
      "\n",
      "User: Can I return a product I bought 20 days ago?\n",
      "üìä Question #1\n",
      "Assistant: Yes. You‚Äôre within our 30-day return window as long as you have the receipt. If you‚Äôd like, I can help start the return‚Äîcould you share your order number or the email used for the purchase? Also tell me whether you‚Äôd prefer a refund or an exchange.\n",
      "\n",
      "User: Are you open on weekends?\n",
      "üìä Question #2\n",
      "########################################################\n",
      "error: content_type=<Y> | unsupported type\n",
      "########################################################\n",
      "Assistant: {'error': 'content_type=<Y> | unsupported type'}\n",
      "\n",
      "User: What about the return policy again?\n",
      "üìä Question #3\n",
      "########################################################\n",
      "error: content_type=<Y> | unsupported type\n",
      "########################################################\n",
      "Assistant: {'error': 'content_type=<Y> | unsupported type'}\n",
      "\n",
      "‚úÖ Total questions asked: 3\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: Limited-Memory Support Agent\n",
    "\n",
    "# Company policies to preload\n",
    "company_policies = [\n",
    "    {'role': 'user', 'content': [{'text':'What is your return policy?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'We offer 30-day returns for all products with receipt.'}]},\n",
    "    {'role': 'user', 'content': [{'text':'What are your business hours?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'We are open Monday-Friday 9am-6pm EST.'}]},\n",
    "]\n",
    "\n",
    "class SupportAgent(Agent):\n",
    "    \"\"\"Support agent that tracks questions.\"\"\"\n",
    "    \n",
    "    async def process(self, context: ExecutionContext):\n",
    "        # Track question count in state\n",
    "        question_count = context.state.get('question_count', 0) + 1\n",
    "        context.state['question_count'] = question_count\n",
    "        \n",
    "        print(f\"üìä Question #{question_count}\")\n",
    "        \n",
    "        result = await super().process(context)\n",
    "        return result\n",
    "\n",
    "# Create configuration\n",
    "support_config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='support_agent',\n",
    "    system_prompt='You are a friendly customer support agent. Help users with their questions about our products and policies.',\n",
    "    preload_messages=company_policies,\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=6\n",
    "    ),\n",
    "    initial_state=default_node_state(question_count=0)\n",
    ")\n",
    "\n",
    "support_agent = SupportAgent(config=support_config)\n",
    "\n",
    "print(\"=== Solution 1: Limited-Memory Support Agent ===\")\n",
    "print()\n",
    "\n",
    "# Test conversation\n",
    "questions = [\n",
    "    \"Can I return a product I bought 20 days ago?\",\n",
    "    \"Are you open on weekends?\",\n",
    "    \"What about the return policy again?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"User: {q}\")\n",
    "    # Use structured content format to match preloaded messages\n",
    "    result = await support_agent.do({'messages': [{'role': 'user', 'content': [{'text': q}]}]})\n",
    "    print(f\"Assistant: {result.content}\")\n",
    "    print()\n",
    "\n",
    "print(f\"‚úÖ Total questions asked: {support_agent._state['question_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Solution 2: Stateless FAQ Agent ===\n",
      "\n",
      "Q1: How do I contact support?\n",
      "A1: I don‚Äôt have memory of past questions, but I can help you contact support. Here are general ways to reach them and how to speed things up:\n",
      "\n",
      "Common contact options\n",
      "- Website: Look for a ‚ÄúContact Us‚Äù or ‚ÄúSupport‚Äù link (usually in the footer).\n",
      "- Phone: Most companies list a support phone number with hours.\n",
      "- Email: Support or help@ or similar email address.\n",
      "- Live chat: Often available on the support page or app.\n",
      "- Help center / ticket system: Submit a support ticket with details of your issue.\n",
      "- Social media: Some brands respond via Twitter/X or Facebook messages.\n",
      "- In-app support: If you‚Äôre using a app, check Help > Contact Support or Support/Feedback.\n",
      "\n",
      "What to have ready\n",
      "- Your account or order number (if applicable)\n",
      "- A clear description of the issue\n",
      "- Steps to reproduce the problem\n",
      "- Your device/OS and app version (if it‚Äôs a mobile app)\n",
      "- Screenshots or screen recordings (optional but helpful)\n",
      "- Any relevant error messages\n",
      "\n",
      "How to phrase a ticket or message\n",
      "- Brief summary: ‚ÄúIssue with [product/service] after [time/event]‚Äù\n",
      "- Details: what happened, what you expected, and what you tried\n",
      "- Attachments: include screenshots or logs if possible\n",
      "\n",
      "If you tell me the company or product you‚Äôre trying to reach, I can give you the exact contact method (phone number, email, and hours) and the best channel for your issue.\n",
      "\n",
      "Q2: What about billing questions?\n",
      "A2: I don‚Äôt have memory of past questions, but I can help with general billing questions. Here‚Äôs a quick starter guide. If you tell me the specific product or service, I can tailor it.\n",
      "\n",
      "- Finding and downloading invoices/receipts\n",
      "  - In your account, go to Billing or Payments. You can usually view billing history and download invoices or receipts for each charge.\n",
      "\n",
      "- Payment methods\n",
      "  - You can add, remove, or update credit/debit cards, PayPal, or other supported methods in the Billing/Payments section. Look for an ‚ÄúAdd payment method‚Äù or ‚ÄúManage payment methods‚Äù option.\n",
      "\n",
      "- Subscriptions and plan changes\n",
      "  - Upgrades/downgrades: often apply at the next billing cycle or immediately, depending on the service.\n",
      "  - Cancellations: you can cancel or pause a subscription in the Billing area; you‚Äôll usually retain access until the current period ends.\n",
      "  - Proration: some services prorate charges when you change plans mid-cycle.\n",
      "\n",
      "- Taxes and billing information\n",
      "  - You can update billing address, tax IDs (e.g., VAT/GST), and country in your profile or Billing settings so invoices reflect the correct details.\n",
      "\n",
      "- Refunds and disputes\n",
      "  - Most services have a refund policy and a process to file a dispute or request a refund. Look for ‚ÄúBilling help,‚Äù ‚ÄúRefunds,‚Äù or ‚ÄúContact support‚Äù in the help center.\n",
      "\n",
      "- Overages and credits\n",
      "  - If you exceed plan limits, you may incur overage charges or be prompted to upgrade. Some services offer credits or usage alerts.\n",
      "\n",
      "- Failed payments\n",
      "  - If a payment fails, you‚Äôll typically receive a notification. Update your payment method and try again; there can be a grace period before service is interrupted.\n",
      "\n",
      "- Security and privacy\n",
      "  - Billing data is usually protected with encryption and PCI-compliant processes. Don‚Äôt share payment details in public messages.\n",
      "\n",
      "How I can help next\n",
      "- Tell me the product or service you‚Äôre using, and describe the exact billing issue (e.g., ‚ÄúI can‚Äôt download an invoice,‚Äù or ‚ÄúI was charged twice,‚Äù or ‚ÄúHow do I cancel?‚Äù).\n",
      "- If you can share non-sensitive details (like invoice number or date) I can guide you on where to look or what steps to take. I don‚Äôt have access to your account, but I can walk you through the process.\n",
      "\n",
      "Q3: What was my first question?\n",
      "A3: I don‚Äôt have memory of past interactions, so I can‚Äôt tell you what your first question was. If you‚Äôd like, you can tell me what you want to ask now, or you can check the chat history on your side to see earlier messages.\n",
      "\n",
      "‚úÖ Agent correctly doesn't remember previous questions (NULL policy)\n"
     ]
    }
   ],
   "source": [
    "# Solution 2: Stateless FAQ Agent\n",
    "\n",
    "# FAQ examples to preload\n",
    "faq_examples = [\n",
    "    {'role': 'user', 'content': [{'text':'How do I reset my password?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'Click \"Forgot Password\" on the login page and follow the email instructions.'}]},\n",
    "    {'role': 'user', 'content': [{'text':'Where can I find my order history?'}]},\n",
    "    {'role': 'assistant', 'content': [{'text':'Go to Account > Orders to view your order history.'}]},\n",
    "]\n",
    "\n",
    "# Create stateless FAQ agent\n",
    "faq_config = AgentConfig(\n",
    "    model=OpenAIModel(model_id='gpt-5-nano'),\n",
    "    name='faq_bot',\n",
    "    system_prompt=\"\"\"You are a FAQ bot that answers questions independently.\n",
    "You do NOT remember previous questions. Each question is answered in isolation.\n",
    "If asked about previous questions, clearly state that you don't have memory.\"\"\",\n",
    "    preload_messages=faq_examples,\n",
    "    memory_config=MemoryConfig(policy=MemoryPolicyType.NULL),\n",
    ")\n",
    "\n",
    "faq_agent = Agent(faq_config)\n",
    "\n",
    "print(\"=== Solution 2: Stateless FAQ Agent ===\")\n",
    "print()\n",
    "\n",
    "# Test with follow-up questions\n",
    "test_questions = [\n",
    "    \"How do I contact support?\",\n",
    "    \"What about billing questions?\",\n",
    "    \"What was my first question?\",  # Should NOT remember\n",
    "]\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    # Use structured content format to match preloaded messages\n",
    "    result = await faq_agent.do({'messages': [{'role': 'user', 'content': [{'text': q}]}]})\n",
    "    print(f\"A{i}: {result.content}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ Agent correctly doesn't remember previous questions (NULL policy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Solution 3: Custom Memory Research Assistant ===\n",
      "\n",
      "Testing custom memory filtering:\n",
      "  üìä Memory: 8 ‚Üí 5 messages\n",
      "     Research: 2, Recent: 3\n",
      "\n",
      "Retained messages:\n",
      "  üî¨ user: [{'text': 'Research topic: Quantum computing'}]...\n",
      "  üî¨ user: [{'text': 'Research topic: Machine learning'}]...\n",
      "  üí¨ assistant: [{'text': 'Noted: ML research'}]...\n",
      "  üí¨ user: [{'text': 'Tell me a joke'}]...\n",
      "  üí¨ assistant: [{'text': 'Why did the...'}]...\n",
      "\n",
      "‚úÖ Custom policy retains all research messages + recent regular messages\n"
     ]
    }
   ],
   "source": [
    "# Solution 3: Custom Memory Policy for Research Assistant\n",
    "\n",
    "def research_memory_policy(messages: list[dict[str, Any]]) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Keep all 'research' tagged messages + last 3 regular messages.\n",
    "    \"\"\"\n",
    "    research_messages = [\n",
    "        m for m in messages \n",
    "        if m.get('metadata', {}).get('type') == 'research'\n",
    "    ]\n",
    "    \n",
    "    regular_messages = [\n",
    "        m for m in messages \n",
    "        if m.get('metadata', {}).get('type') != 'research'\n",
    "    ]\n",
    "    \n",
    "    recent_regular = regular_messages[-3:]\n",
    "    \n",
    "    result = research_messages + recent_regular\n",
    "    \n",
    "    print(f\"  üìä Memory: {len(messages)} ‚Üí {len(result)} messages\")\n",
    "    print(f\"     Research: {len(research_messages)}, Recent: {len(recent_regular)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create research assistant\n",
    "research_config = AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='research_assistant',\n",
    "    system_prompt='You are a research assistant. Prioritize research-related information.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.CUSTOM,\n",
    "        callable=research_memory_policy\n",
    "    ),\n",
    ")\n",
    "\n",
    "research_agent = Agent(research_config)\n",
    "\n",
    "print(\"=== Solution 3: Custom Memory Research Assistant ===\")\n",
    "print()\n",
    "\n",
    "# Simulate messages with metadata\n",
    "test_messages = [\n",
    "    {'role': 'user', 'content': [{'text':'Research topic: Quantum computing'}], 'metadata': {'type': 'research'}},\n",
    "    {'role': 'assistant', 'content': [{'text':'Noted: Quantum computing research'}]},\n",
    "    {'role': 'user', 'content': [{'text':'What\\'s the weather?'}], 'metadata': {'type': 'casual'}},\n",
    "    {'role': 'assistant', 'content': [{'text':'Sunny today'}]},\n",
    "    {'role': 'user', 'content': [{'text':'Research topic: Machine learning'}], 'metadata': {'type': 'research'}},\n",
    "    {'role': 'assistant', 'content': [{'text':'Noted: ML research'}]},\n",
    "    {'role': 'user', 'content': [{'text':'Tell me a joke'}], 'metadata': {'type': 'casual'}},\n",
    "    {'role': 'assistant', 'content': [{'text':'Why did the...'}]},\n",
    "]\n",
    "\n",
    "print(\"Testing custom memory filtering:\")\n",
    "filtered = research_memory_policy(test_messages)\n",
    "print()\n",
    "\n",
    "print(\"Retained messages:\")\n",
    "for msg in filtered:\n",
    "    msg_type = msg.get('metadata', {}).get('type', 'unknown')\n",
    "    marker = \"üî¨\" if msg_type == 'research' else \"üí¨\"\n",
    "    print(f\"  {marker} {msg['role']}: {msg['content'][:40]}...\")\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Custom policy retains all research messages + recent regular messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Memory Policy Comparison\n",
    "\n",
    "print(\"=== Solution 4: Memory Policy Comparison ===\")\n",
    "print()\n",
    "\n",
    "# Create three agents with different memory policies\n",
    "agent_null = Agent(AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='agent_null',\n",
    "    system_prompt='You are an assistant.',\n",
    "    memory_config=MemoryConfig(policy=MemoryPolicyType.NULL),\n",
    "))\n",
    "\n",
    "agent_small_window = Agent(AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='agent_small',\n",
    "    system_prompt='You are an assistant.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=4\n",
    "    ),\n",
    "))\n",
    "\n",
    "agent_large_window = Agent(AgentConfig(\n",
    "    model=EchoModel(),\n",
    "    name='agent_large',\n",
    "    system_prompt='You are an assistant.',\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=10\n",
    "    ),\n",
    "))\n",
    "\n",
    "# Test conversation\n",
    "conversation = [\n",
    "    \"My name is Bob.\",\n",
    "    \"I live in New York.\",\n",
    "    \"I work as a software engineer.\",\n",
    "    \"I like pizza.\",\n",
    "    \"What's my name?\",\n",
    "    \"Where do I live?\",\n",
    "    \"What do I do for work?\",\n",
    "]\n",
    "\n",
    "agents = [\n",
    "    ('NULL Policy', agent_null),\n",
    "    ('Rolling Window (4)', agent_small_window),\n",
    "    ('Rolling Window (10)', agent_large_window),\n",
    "]\n",
    "\n",
    "for agent_name, agent in agents:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {agent_name}\")\n",
    "    print(f\"Memory: {agent.config.memory_config.policy}\")\n",
    "    if agent.config.memory_config.policy == MemoryPolicyType.ROLLING_WINDOW:\n",
    "        print(f\"Window: {agent.config.memory_config.window}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for msg in conversation:\n",
    "        # Use structured content format\n",
    "        result = await agent.do({'messages': [{'role': 'user', 'content': [{'text': msg}]}]})\n",
    "        if '?' in msg:\n",
    "            print(f\"Q: {msg}\")\n",
    "            print(f\"A: {result.content}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Comparison Analysis:\")\n",
    "print(\"  NULL: Cannot answer any reference questions (no memory)\")\n",
    "print(\"  Small Window (4): Remembers recent info, may forget earlier details\")\n",
    "print(\"  Large Window (10): Remembers all conversation details\")\n",
    "print()\n",
    "print(\"‚úÖ Different memory policies suit different use cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary & Next Steps\n",
    "\n",
    "### Congratulations! üéâ\n",
    "\n",
    "You've mastered agent configuration and memory management in Spark! You now understand how to configure agents for different use cases, manage conversation memory effectively, and coordinate multiple agents using GraphState.\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "‚úÖ **AgentConfig Mastery**\n",
    "- Centralized configuration with AgentConfig\n",
    "- Type-safe configuration with Pydantic\n",
    "- All configuration options and their purposes\n",
    "- Benefits over scattered configuration\n",
    "\n",
    "‚úÖ **Memory Management**\n",
    "- Three memory policies: NULL, ROLLING_WINDOW, CUSTOM\n",
    "- When to use each policy\n",
    "- How to configure memory windows\n",
    "- Custom memory policies for advanced use cases\n",
    "\n",
    "‚úÖ **State vs Memory vs GraphState**\n",
    "- Critical distinctions between node state, memory, and graph state\n",
    "- When to use each mechanism\n",
    "- How they affect LLM behavior and token usage\n",
    "- Best practices for all three\n",
    "\n",
    "‚úÖ **Advanced Patterns**\n",
    "- Preloading messages for context initialization\n",
    "- Custom memory policies for complex requirements\n",
    "- Using GraphState for multi-agent coordination\n",
    "- Thread-safe state sharing across nodes\n",
    "\n",
    "### Key Patterns to Remember:\n",
    "\n",
    "```python\n",
    "# Stateless agent\n",
    "config = AgentConfig(\n",
    "    model=model,\n",
    "    memory_config=MemoryConfig(policy=MemoryPolicyType.NULL)\n",
    ")\n",
    "\n",
    "# Conversational agent\n",
    "config = AgentConfig(\n",
    "    model=model,\n",
    "    memory_config=MemoryConfig(\n",
    "        policy=MemoryPolicyType.ROLLING_WINDOW,\n",
    "        window=10\n",
    "    )\n",
    ")\n",
    "\n",
    "# Multi-agent workflow with GraphState\n",
    "graph = Graph(\n",
    "    start=agent1,\n",
    "    initial_state={'counter': 0, 'results': []}\n",
    ")\n",
    "\n",
    "# In agents' process() method\n",
    "async def process(self, context):\n",
    "    # Node state (local to this agent)\n",
    "    context.state['my_count'] = 1\n",
    "    \n",
    "    # Graph state (shared across all agents)\n",
    "    if context.graph_state:\n",
    "        total = await context.graph_state.get('counter', 0)\n",
    "        await context.graph_state.set('counter', total + 1)\n",
    "    \n",
    "    return {'done': True}\n",
    "```\n",
    "\n",
    "### üìö Related Resources:\n",
    "\n",
    "- Example files: `e006_agent_with_config.py`, `e009_tool_agent.py`\n",
    "- Source: `spark/agents/config.py`, `spark/agents/memory.py`\n",
    "- Tutorial 4: Your First AI Agent (review if needed)\n",
    "- Tutorial 6: LLM-Powered Routing (builds on this)\n",
    "- CLAUDE.md: GraphState documentation in Core Architecture section\n",
    "\n",
    "### üöÄ Next Tutorial: Tools - Extending Agent Capabilities\n",
    "\n",
    "In **Tutorial 8**, you'll learn how to:\n",
    "- Create custom tools for agents\n",
    "- Understand tool calling mechanics\n",
    "- Configure tool choice strategies\n",
    "- Debug tool execution\n",
    "- Build multi-tool agents\n",
    "- Implement complex tool-based workflows\n",
    "\n",
    "### üîß Before You Move On:\n",
    "\n",
    "Make sure you can:\n",
    "1. ‚úÖ Create agents with AgentConfig\n",
    "2. ‚úÖ Choose and configure appropriate memory policies\n",
    "3. ‚úÖ Distinguish between state, memory, and GraphState\n",
    "4. ‚úÖ Use preloaded messages effectively\n",
    "5. ‚úÖ Implement custom memory policies\n",
    "6. ‚úÖ Use GraphState to coordinate multiple agents in a graph\n",
    "\n",
    "### üéì Tutorial Series Progress:\n",
    "\n",
    "- ‚úÖ **Tutorial 1: Hello Spark** - Basic nodes\n",
    "- ‚úÖ **Tutorial 2: Batch Processing** - Parallel execution\n",
    "- ‚úÖ **Tutorial 3: Simple Flows** - Graph basics\n",
    "- ‚úÖ **Tutorial 4: Your First AI Agent** - Agent fundamentals\n",
    "- ‚úÖ **Tutorial 5: Conditional Routing** - Decision making\n",
    "- ‚úÖ **Tutorial 6: Agent Config & Memory** - *You are here!* üéØ\n",
    "- ‚û°Ô∏è **Tutorial 7: Tools** - Extending agents\n",
    "\n",
    "### üåü Pro Tips:\n",
    "\n",
    "- **Start with ROLLING_WINDOW** for most conversational agents\n",
    "- **Monitor token usage** to optimize window size\n",
    "- **Use NULL** for stateless, high-performance APIs\n",
    "- **Consider CUSTOM** when standard policies don't fit\n",
    "- **Use node state** for per-agent tracking\n",
    "- **Use memory** for LLM conversation context\n",
    "- **Use GraphState** for multi-agent coordination\n",
    "- **Preload context** to save setup time and tokens\n",
    "- **Test memory behavior** with realistic conversations\n",
    "- **Document your decisions** about state/memory/GraphState\n",
    "- **Always check** `if context.graph_state` before using it\n",
    "\n",
    "### üéØ Challenge Before Next Tutorial:\n",
    "\n",
    "Build a \"Multi-Agent Workflow\" that:\n",
    "1. Has two agents: Classifier and Processor\n",
    "2. Classifier uses NULL memory (stateless)\n",
    "3. Processor uses rolling window memory (window=6)\n",
    "4. Both agents update a shared counter in GraphState\n",
    "5. GraphState tracks total classifications and total processing time\n",
    "6. Each agent tracks its own invocation count in node state\n",
    "\n",
    "This will prepare you for adding tools in Tutorial 8!\n",
    "\n",
    "---\n",
    "\n",
    "**You're now ready to build production-ready multi-agent workflows with proper configuration, memory management, and state coordination!** üöÄ\n",
    "\n",
    "Have questions or feedback? Check the Spark documentation or open an issue on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
