{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Simple Flows and Graph Basics\n",
    "\n",
    "**Difficulty:** Beginner | **Time:** 20 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Connect multiple nodes into a flow\n",
    "- Understand edge creation with `>>` operator\n",
    "- Create and run a graph\n",
    "- Work with ExecutionContext\n",
    "- Use `context.inputs` and return outputs correctly\n",
    "- Pass data between nodes via outputs (not node state!)\n",
    "- **NEW**: Use graph state for globally shared data across all nodes\n",
    "- **Understand the three storage types**: outputs, node state, and graph state\n",
    "\n",
    "## Real-World Use Case\n",
    "\n",
    "Imagine you're building a data processing pipeline: first you collect data, then you clean it, transform it, analyze it, and finally generate a report. Each step depends on the previous one, and data needs to flow between them. Spark's graphs let you create these workflows as connected nodes, where each node does one specific job and passes results to the next **via outputs** (not state!).\n",
    "\n",
    "Additionally, you might need to track metrics across the entire pipeline (like total items processed, error counts, or global flags). For this, Spark provides **graph state** - a globally shared state accessible to all nodes, perfect for coordination and aggregation.\n",
    "\n",
    "In this tutorial, we'll build several example flows to understand how nodes work together in graphs, and we'll learn the critical distinction between the three storage types: outputs for sequential flow, node state for private data, and graph state for global coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts\n",
    "\n",
    "### Graph Architecture\n",
    "\n",
    "A **Graph** in Spark is a collection of connected **Nodes** that process data in sequence:\n",
    "\n",
    "```\n",
    "Input Data â†’ Node 1 â†’ Node 2 â†’ Node 3 â†’ Output\n",
    "             (outputs flow â†’)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Node**: A processing unit that takes input and produces output\n",
    "2. **Edge**: Connection between nodes that determines data flow\n",
    "3. **Graph**: Container that orchestrates node execution\n",
    "4. **ExecutionContext**: Runtime environment with inputs, node state, metadata, and graph state\n",
    "\n",
    "### CRITICAL: The Actor Model - Three Types of Data Storage\n",
    "\n",
    "**Spark implements the Actor Model with three distinct data storage mechanisms:**\n",
    "\n",
    "1. **Outputs FLOW between nodes**: A node's return value becomes the next node's `context.inputs`\n",
    "   - Use for: Data that's transformed/created by a node and consumed by specific successors\n",
    "   - Flows: One node â†’ Next node(s)\n",
    "\n",
    "2. **Node State is NODE-LOCAL**: Each node's `context.state` is private and does NOT flow\n",
    "   - Use for: A node's own persistent data (e.g., invocation counts, internal cache)\n",
    "   - Flows: Nowhere, persists only for that node across invocations\n",
    "\n",
    "3. **Graph State is GLOBALLY SHARED**: `context.graph_state` is accessible to ALL nodes\n",
    "   - Use for: Shared counters, accumulated results, coordination flags, global configuration\n",
    "   - Flows: Everywhere, accessible by all nodes in the graph\n",
    "\n",
    "```python\n",
    "# âŒ WRONG - Node state doesn't flow\n",
    "async def process(self, context):\n",
    "    context.state['data'] = value  # Next node won't see this!\n",
    "\n",
    "# âœ… RIGHT - Outputs flow to next nodes\n",
    "async def process(self, context):\n",
    "    return {'data': value}  # Next node receives this as inputs\n",
    "\n",
    "# âœ… RIGHT - Graph state is shared across all nodes\n",
    "async def process(self, context):\n",
    "    await context.graph_state.set('counter', 42)  # All nodes can access this!\n",
    "```\n",
    "\n",
    "### When to Use Each Storage Type\n",
    "\n",
    "**Use Outputs when:**\n",
    "- Data is transformed by a node and only its direct successors need it\n",
    "- Each node in the pipeline adds/modifies the data\n",
    "- The data flow is linear or tree-like\n",
    "\n",
    "**Use Node State when:**\n",
    "- Tracking a node's own invocation count, retry attempts, or internal cache\n",
    "- The data is specific to one node's behavior across multiple executions\n",
    "- Other nodes should NOT see this data\n",
    "\n",
    "**Use Graph State when:**\n",
    "- Multiple nodes need to read/write the same data (counters, flags, collections)\n",
    "- Coordinating behavior across distant nodes in the graph\n",
    "- Accumulating results that don't need to flow through every node\n",
    "- Sharing configuration or context that all nodes need\n",
    "\n",
    "### The `>>` Operator\n",
    "\n",
    "The `>>` operator creates edges between nodes:\n",
    "\n",
    "```python\n",
    "node1 >> node2  # Output of node1 flows to node2's inputs\n",
    "```\n",
    "\n",
    "### Auto-Discovery\n",
    "\n",
    "Spark automatically discovers all connected nodes when you create a graph with just the start node:\n",
    "\n",
    "```python\n",
    "graph = Graph(start=start_node)  # Auto-discovers all connected nodes\n",
    "```\n",
    "\n",
    "### Graph State Initialization\n",
    "\n",
    "You can initialize graph state when creating a graph:\n",
    "\n",
    "```python\n",
    "# Empty graph state (default)\n",
    "graph = Graph(start=start_node)\n",
    "\n",
    "# With initial values\n",
    "graph = Graph(start=start_node, initial_state={'counter': 0, 'results': []})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary classes and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core Spark classes\n",
    "from spark.nodes import Node\n",
    "from spark.graphs import Graph\n",
    "from spark.utils import arun\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Your First Simple Flow\n",
    "\n",
    "Let's start with a basic 2-node flow: a data generator connected to a data processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Node):\n",
    "    \"\"\"Generates sample data.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        print(\"ðŸ­ Generating data...\")\n",
    "        data = {\n",
    "            'numbers': [1, 2, 3, 4, 5],\n",
    "            'message': 'Hello from DataGenerator!'\n",
    "        }\n",
    "        print(f\"âœ… Generated: {data}\")\n",
    "        return data\n",
    "\n",
    "class DataProcessor(Node):\n",
    "    \"\"\"Processes data from previous node.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Access input from previous node\n",
    "        input_data = context.inputs.content\n",
    "        numbers = input_data.get('numbers', [])\n",
    "        message = input_data.get('message', '')\n",
    "        \n",
    "        print(f\"ðŸ”§ Processing: {message}\")\n",
    "        \n",
    "        # Process the numbers (calculate sum and average)\n",
    "        total = sum(numbers)\n",
    "        average = total / len(numbers) if numbers else 0\n",
    "        \n",
    "        result = {\n",
    "            'original_message': message,\n",
    "            'numbers': numbers,\n",
    "            'sum': total,\n",
    "            'average': average,\n",
    "            'processed_at': time.time()\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Processed result: sum={total}, average={average:.2f}\")\n",
    "        return result\n",
    "\n",
    "# Create nodes\n",
    "generator = DataGenerator()\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Connect nodes with >> operator\n",
    "generator >> processor\n",
    "\n",
    "# Create graph with auto-discovery\n",
    "graph = Graph(start=generator)\n",
    "\n",
    "print(\"=== Example 1: Simple 2-Node Flow ===\")\n",
    "result = await graph.run()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Final result: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Step Pipeline\n",
    "\n",
    "Let's build a longer pipeline with multiple processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCollector(Node):\n",
    "    \"\"\"Collects and prepares text for processing.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Get initial text from context inputs\n",
    "        text = context.inputs.content.get('text', 'Hello World')\n",
    "        \n",
    "        print(f\"ðŸ“ Collecting text: '{text}'\")\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'word_count': len(text.split()),\n",
    "            'char_count': len(text)\n",
    "        }\n",
    "\n",
    "class TextCleaner(Node):\n",
    "    \"\"\"Cleans and normalizes text.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        text = data.get('original_text', '')\n",
    "        \n",
    "        print(f\"ðŸ§¹ Cleaning text: '{text}'\")\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned = text.strip().lower()\n",
    "        \n",
    "        result = data.copy()\n",
    "        result.update({\n",
    "            'cleaned_text': cleaned,\n",
    "            'has_numbers': any(char.isdigit() for char in text),\n",
    "            'has_special_chars': any(not char.isalnum() and not char.isspace() for char in text)\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "class TextAnalyzer(Node):\n",
    "    \"\"\"Analyzes text and extracts insights.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        text = data.get('cleaned_text', '')\n",
    "        \n",
    "        print(f\"ðŸ” Analyzing text: '{text}'\")\n",
    "        \n",
    "        # Analyze the text\n",
    "        vowels = sum(1 for char in text if char in 'aeiou')\n",
    "        consonants = sum(1 for char in text if char.isalpha() and char not in 'aeiou')\n",
    "        \n",
    "        # Extract words and analyze\n",
    "        words = text.split()\n",
    "        longest_word = max(words, key=len) if words else ''\n",
    "        \n",
    "        result = data.copy()\n",
    "        result.update({\n",
    "            'vowel_count': vowels,\n",
    "            'consonant_count': consonants,\n",
    "            'longest_word': longest_word,\n",
    "            'analysis_complete': True\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "class ReportGenerator(Node):\n",
    "    \"\"\"Generates a final report.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        \n",
    "        print(\"ðŸ“Š Generating report...\")\n",
    "        \n",
    "        # Create a summary report\n",
    "        report = f\"\"\"\n",
    "=== Text Analysis Report ===\n",
    "Original Text: {data.get('original_text', 'N/A')}\n",
    "Cleaned Text: {data.get('cleaned_text', 'N/A')}\n",
    "Word Count: {data.get('word_count', 0)}\n",
    "Character Count: {data.get('char_count', 0)}\n",
    "Vowels: {data.get('vowel_count', 0)}\n",
    "Consonants: {data.get('consonant_count', 0)}\n",
    "Longest Word: {data.get('longest_word', 'N/A')}\n",
    "Contains Numbers: {data.get('has_numbers', False)}\n",
    "Contains Special Chars: {data.get('has_special_chars', False)}\n",
    "=============================\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        print(report)\n",
    "        \n",
    "        return {\n",
    "            'report': report,\n",
    "            'summary': {\n",
    "                'original_length': data.get('char_count', 0),\n",
    "                'words_processed': data.get('word_count', 0),\n",
    "                'analysis_steps': 4\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create the pipeline\n",
    "collector = TextCollector()\n",
    "cleaner = TextCleaner()\n",
    "analyzer = TextAnalyzer()\n",
    "reporter = ReportGenerator()\n",
    "\n",
    "# Connect all nodes in sequence\n",
    "collector >> cleaner >> analyzer >> reporter\n",
    "\n",
    "# Create graph\n",
    "pipeline = Graph(start=collector)\n",
    "\n",
    "print(\"=== Example 2: Multi-Step Text Pipeline ===\")\n",
    "print(\"Processing: 'Hello Spark Framework 2024!'\")\n",
    "\n",
    "# Run with initial input\n",
    "result = await pipeline.run({'text': 'Hello Spark Framework 2024!'})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Pipeline completed!\")\n",
    "print(f\"Summary: {result.content['summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Understanding the Three Storage Types\n",
    "\n",
    "This example demonstrates both INCORRECT and CORRECT patterns for each storage type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Example 3A: INCORRECT Pattern - Trying to Share State ===\" )\n",
    "print(\"âš ï¸  This demonstrates what NOT to do!\\\\n\")\n",
    "\n",
    "class StateSharer(Node):\n",
    "    \"\"\"Tries to share data via state (INCORRECT!).\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "    \n",
    "    async def process(self, context):\n",
    "        print(f\"ðŸ” {self.name}:\")\n",
    "        print(f\"  My state keys: {list(context.state.keys())}\")\n",
    "        \n",
    "        # INCORRECT: Trying to add data to state for other nodes\n",
    "        context.state[f'{self.name}_timestamp'] = time.time()\n",
    "        context.state[f'{self.name}_processed'] = True\n",
    "        \n",
    "        return {'node': self.name, 'attempt': 'state_sharing'}\n",
    "\n",
    "class StateReader(Node):\n",
    "    \"\"\"Tries to read state from previous nodes (INCORRECT!).\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        print(f\"\\\\nðŸ“Š StateReader trying to read shared state:\")\n",
    "        print(f\"  My state keys: {list(context.state.keys())}\")\n",
    "        \n",
    "        # Try to count processed nodes (won't work!)\n",
    "        processed_nodes = [k for k in context.state.keys() if k.endswith('_processed')]\n",
    "        print(f\"  âŒ Found {len(processed_nodes)} processed nodes (expected 3, got 0!)\")\n",
    "        print(f\"  âŒ Previous nodes' state changes are NOT visible here!\")\n",
    "        \n",
    "        return {'found_nodes': len(processed_nodes)}\n",
    "\n",
    "# Create chain\n",
    "sharer1 = StateSharer(\"Node1\")\n",
    "sharer2 = StateSharer(\"Node2\")\n",
    "sharer3 = StateSharer(\"Node3\")\n",
    "reader = StateReader()\n",
    "\n",
    "sharer1 >> sharer2 >> sharer3 >> reader\n",
    "\n",
    "# Run\n",
    "graph1 = Graph(start=sharer1)\n",
    "result1 = await graph1.run({'message': 'test'})\n",
    "\n",
    "print(f\"\\\\nðŸŽ¯ Result: Only found {result1.content['found_nodes']} nodes\")\n",
    "print(\"âŒ This pattern FAILS because state is node-local!\\\\n\")\n",
    "\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"=== Example 3B: CORRECT Pattern - Using Outputs ===\" )\n",
    "print(\"âœ… This shows using outputs for linear data flow!\\\\n\")\n",
    "\n",
    "class DataCollector(Node):\n",
    "    \"\"\"Collects data correctly using outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Get data from previous node via inputs\n",
    "        input_data = context.inputs.content\n",
    "        processing_chain = input_data.get('processing_chain', [])\n",
    "        \n",
    "        print(f\"ðŸ“ {self.name}:\")\n",
    "        print(f\"  Received chain: {processing_chain}\")\n",
    "        \n",
    "        # Use state for THIS node's own data (e.g., invocation count)\n",
    "        if 'invocation_count' not in context.state:\n",
    "            context.state['invocation_count'] = 0\n",
    "        context.state['invocation_count'] += 1\n",
    "        \n",
    "        # CORRECT: Return data via outputs to share with next nodes\n",
    "        new_chain = processing_chain + [{\n",
    "            'node': self.name,\n",
    "            'timestamp': time.time(),\n",
    "            'invocations': context.state['invocation_count']\n",
    "        }]\n",
    "        \n",
    "        return {\n",
    "            'processing_chain': new_chain,\n",
    "            'last_node': self.name\n",
    "        }\n",
    "\n",
    "class ChainAnalyzer(Node):\n",
    "    \"\"\"Analyzes the processing chain received via inputs.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Get data from previous nodes via inputs (CORRECT!)\n",
    "        input_data = context.inputs.content\n",
    "        chain = input_data.get('processing_chain', [])\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š ChainAnalyzer received data:\")\n",
    "        print(f\"  âœ… Processing chain length: {len(chain)}\")\n",
    "        print(f\"  âœ… Nodes that processed this: {[item['node'] for item in chain]}\")\n",
    "        \n",
    "        return {\n",
    "            'chain': chain,\n",
    "            'total_nodes': len(chain),\n",
    "            'analysis_complete': True\n",
    "        }\n",
    "\n",
    "# Create chain\n",
    "collector1 = DataCollector(\"Collector1\")\n",
    "collector2 = DataCollector(\"Collector2\")\n",
    "collector3 = DataCollector(\"Collector3\")\n",
    "analyzer = ChainAnalyzer()\n",
    "\n",
    "collector1 >> collector2 >> collector3 >> analyzer\n",
    "\n",
    "# Run\n",
    "graph2 = Graph(start=collector1)\n",
    "result2 = await graph2.run({'processing_chain': []})\n",
    "\n",
    "print(f\"\\\\nðŸŽ¯ Final Result:\")\n",
    "print(f\"  âœ… Correctly tracked {result2.content['total_nodes']} nodes\")\n",
    "print(f\"  âœ… Chain: {[item['node'] for item in result2.content['chain']]}\")\n",
    "\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"=== Example 3C: BEST Pattern - Using Graph State ===\" )\n",
    "print(\"âœ… This shows using graph state for shared data!\\\\n\")\n",
    "\n",
    "class GraphStateCollector(Node):\n",
    "    \"\"\"Collects data using graph state for coordination.\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Use node state for THIS node's own data\n",
    "        if 'invocation_count' not in context.state:\n",
    "            context.state['invocation_count'] = 0\n",
    "        context.state['invocation_count'] += 1\n",
    "        \n",
    "        # Use graph state for shared coordination data\n",
    "        processing_chain = await context.graph_state.get('processing_chain', [])\n",
    "        \n",
    "        print(f\"ðŸ“ {self.name}:\")\n",
    "        print(f\"  Graph state chain: {processing_chain}\")\n",
    "        \n",
    "        # Append to the shared processing chain\n",
    "        processing_chain.append({\n",
    "            'node': self.name,\n",
    "            'timestamp': time.time(),\n",
    "            'invocations': context.state['invocation_count']\n",
    "        })\n",
    "        \n",
    "        await context.graph_state.set('processing_chain', processing_chain)\n",
    "        \n",
    "        # Increment shared counter\n",
    "        counter = await context.graph_state.get('counter', 0)\n",
    "        await context.graph_state.set('counter', counter + 1)\n",
    "        \n",
    "        # Return minimal data via outputs (just pass through signal)\n",
    "        return {'processed': True, 'node': self.name}\n",
    "\n",
    "class GraphStateAnalyzer(Node):\n",
    "    \"\"\"Analyzes data from graph state.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        # Access shared graph state directly\n",
    "        chain = await context.graph_state.get('processing_chain', [])\n",
    "        counter = await context.graph_state.get('counter', 0)\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š GraphStateAnalyzer accessing graph state:\")\n",
    "        print(f\"  âœ… Processing chain length: {len(chain)}\")\n",
    "        print(f\"  âœ… Nodes that processed this: {[item['node'] for item in chain]}\")\n",
    "        print(f\"  âœ… Total counter: {counter}\")\n",
    "        \n",
    "        return {\n",
    "            'total_nodes': len(chain),\n",
    "            'total_counter': counter,\n",
    "            'analysis_complete': True\n",
    "        }\n",
    "\n",
    "# Create chain\n",
    "gs_collector1 = GraphStateCollector(\"GSCollector1\")\n",
    "gs_collector2 = GraphStateCollector(\"GSCollector2\")\n",
    "gs_collector3 = GraphStateCollector(\"GSCollector3\")\n",
    "gs_analyzer = GraphStateAnalyzer()\n",
    "\n",
    "gs_collector1 >> gs_collector2 >> gs_collector3 >> gs_analyzer\n",
    "\n",
    "# Run with initial graph state\n",
    "graph3 = Graph(start=gs_collector1, initial_state={'processing_chain': [], 'counter': 0})\n",
    "result3 = await graph3.run({'message': 'test'})\n",
    "\n",
    "print(f\"\\\\nðŸŽ¯ Final Result:\")\n",
    "print(f\"  âœ… Correctly tracked {result3.content['total_nodes']} nodes using graph state\")\n",
    "print(f\"  âœ… Total counter: {result3.content['total_counter']}\")\n",
    "\n",
    "# Show final graph state\n",
    "final_chain = await graph3.get_state('processing_chain')\n",
    "print(f\"  âœ… Final chain from graph state: {[item['node'] for item in final_chain]}\")\n",
    "\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"=== Key Takeaway ===\" )\n",
    "print(\"âŒ DON'T: Try to share data via context.state - it's node-local\")\n",
    "print(\"âœ… DO: Use outputs to pass transformed data to next nodes\")\n",
    "print(\"âœ… DO: Use graph state for shared counters, flags, and coordination\")\n",
    "print(\"âœ… DO: Use node state only for a node's own persistent data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Data Transformation Pipeline\n",
    "\n",
    "Let's build a practical data transformation pipeline that processes structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator(Node):\n",
    "    \"\"\"Validates input data structure.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        \n",
    "        print(f\"âœ… Validating data: {data}\")\n",
    "        \n",
    "        # Validate required fields\n",
    "        required_fields = ['users']\n",
    "        missing_fields = [field for field in required_fields if field not in data]\n",
    "        \n",
    "        if missing_fields:\n",
    "            raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "        \n",
    "        users = data.get('users', [])\n",
    "        \n",
    "        return {\n",
    "            'original_data': data,\n",
    "            'user_count': len(users),\n",
    "            'validation_passed': True,\n",
    "            'users': users\n",
    "        }\n",
    "\n",
    "class DataEnricher(Node):\n",
    "    \"\"\"Enriches data with additional information.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        users = data.get('users', [])\n",
    "        \n",
    "        print(f\"ðŸ”§ Enriching data for {len(users)} users...\")\n",
    "        \n",
    "        # Enrich each user with computed fields\n",
    "        enriched_users = []\n",
    "        for user in users:\n",
    "            enriched_user = user.copy()\n",
    "            \n",
    "            # Add computed fields\n",
    "            name = user.get('name', '')\n",
    "            enriched_user['name_length'] = len(name)\n",
    "            enriched_user['name_uppercase'] = name.upper()\n",
    "            enriched_user['initials'] = ''.join([n[0].upper() for n in name.split() if n])\n",
    "            enriched_user['has_email'] = '@' in user.get('email', '')\n",
    "            \n",
    "            enriched_users.append(enriched_user)\n",
    "        \n",
    "        result = data.copy()\n",
    "        result['enriched_users'] = enriched_users\n",
    "        result['enrichment_complete'] = True\n",
    "        \n",
    "        return result\n",
    "\n",
    "class DataAggregator(Node):\n",
    "    \"\"\"Aggregates and summarizes data.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        users = data.get('enriched_users', [])\n",
    "        \n",
    "        print(f\"ðŸ“Š Aggregating data from {len(users)} users...\")\n",
    "        \n",
    "        # Calculate aggregates\n",
    "        total_name_length = sum(user.get('name_length', 0) for user in users)\n",
    "        users_with_email = sum(1 for user in users if user.get('has_email', False))\n",
    "        \n",
    "        # Group by initial\n",
    "        initials_count = {}\n",
    "        for user in users:\n",
    "            initial = user.get('initials', '')\n",
    "            if initial:\n",
    "                initials_count[initial] = initials_count.get(initial, 0) + 1\n",
    "        \n",
    "        summary = {\n",
    "            'total_users': len(users),\n",
    "            'total_name_length': total_name_length,\n",
    "            'average_name_length': total_name_length / len(users) if users else 0,\n",
    "            'users_with_email': users_with_email,\n",
    "            'email_percentage': (users_with_email / len(users) * 100) if users else 0,\n",
    "            'initials_distribution': initials_count\n",
    "        }\n",
    "        \n",
    "        result = data.copy()\n",
    "        result['summary'] = summary\n",
    "        result['aggregation_complete'] = True\n",
    "        \n",
    "        return result\n",
    "\n",
    "class DataExporter(Node):\n",
    "    \"\"\"Exports data to final format.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        summary = data.get('summary', {})\n",
    "        \n",
    "        print(\"ðŸ“¤ Exporting data...\")\n",
    "        \n",
    "        # Create export format\n",
    "        export_data = {\n",
    "            'export_timestamp': time.time(),\n",
    "            'summary': summary,\n",
    "            'metadata': {\n",
    "                'total_nodes_processed': 4,\n",
    "                'data_pipeline': 'validation -> enrichment -> aggregation -> export'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print formatted summary\n",
    "        print(\"\\nðŸ“‹ Data Processing Summary:\")\n",
    "        print(f\"  Total Users: {summary.get('total_users', 0)}\")\n",
    "        print(f\"  Average Name Length: {summary.get('average_name_length', 0):.1f}\")\n",
    "        print(f\"  Users with Email: {summary.get('users_with_email', 0)} ({summary.get('email_percentage', 0):.1f}%)\")\n",
    "        print(f\"  Initials Distribution: {summary.get('initials_distribution', {})}\")\n",
    "        \n",
    "        return export_data\n",
    "\n",
    "# Sample data\n",
    "sample_users = [\n",
    "    {'name': 'Alice Johnson', 'email': 'alice@example.com'},\n",
    "    {'name': 'Bob Smith', 'email': 'bob@example.com'},\n",
    "    {'name': 'Charlie Brown', 'email': ''},\n",
    "    {'name': 'Diana Prince', 'email': 'diana@themyscira.com'},\n",
    "    {'name': 'Eve Wilson', 'email': 'eve@example.org'}\n",
    "]\n",
    "\n",
    "# Create pipeline\n",
    "validator = DataValidator()\n",
    "enricher = DataEnricher()\n",
    "aggregator = DataAggregator()\n",
    "exporter = DataExporter()\n",
    "\n",
    "# Connect nodes\n",
    "validator >> enricher >> aggregator >> exporter\n",
    "\n",
    "# Create graph\n",
    "data_pipeline = Graph(start=validator)\n",
    "\n",
    "print(\"=== Example 4: Data Transformation Pipeline ===\")\n",
    "print(f\"Processing {len(sample_users)} users through pipeline...\\n\")\n",
    "\n",
    "# Run pipeline\n",
    "result = await data_pipeline.run({'users': sample_users})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Pipeline completed successfully!\")\n",
    "print(f\"Export created at: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(result.content['export_timestamp']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts Deep Dive\n",
    "\n",
    "### Understanding Data Flow and Storage (CRITICAL!)\n",
    "\n",
    "**The Golden Rule: Three Storage Types for Three Different Needs**\n",
    "\n",
    "Spark provides three distinct mechanisms for data storage, each serving a specific purpose:\n",
    "\n",
    "1. **Outputs**: Flow from one node to its direct successors\n",
    "2. **Node State**: Private to each node, persists across its invocations\n",
    "3. **Graph State**: Shared across all nodes, accessible everywhere\n",
    "\n",
    "### The Three Storage Types in Detail\n",
    "\n",
    "#### 1. Outputs - Sequential Data Flow\n",
    "\n",
    "**Purpose**: Pass transformed data from one node to the next in the pipeline\n",
    "\n",
    "**How it works**:\n",
    "- Node returns a dict from `process()`\n",
    "- Return value becomes next node's `context.inputs.content`\n",
    "- Data flows linearly through the graph edges\n",
    "\n",
    "**When to use**:\n",
    "- Each node transforms or adds to the data\n",
    "- Next node needs the result of previous node\n",
    "- Data flow follows the graph structure\n",
    "\n",
    "```python\n",
    "# Node 1\n",
    "async def process(self, context):\n",
    "    data = context.inputs.content\n",
    "    result = transform(data)\n",
    "    return {'result': result}  # Flows to next node\n",
    "\n",
    "# Node 2 receives it\n",
    "async def process(self, context):\n",
    "    previous_result = context.inputs.content['result']  # From Node 1\n",
    "```\n",
    "\n",
    "#### 2. Node State - Private Persistent Data\n",
    "\n",
    "**Purpose**: Store data that's specific to ONE node across multiple invocations\n",
    "\n",
    "**How it works**:\n",
    "- Accessed via `context.state` (a dict-like object)\n",
    "- Persists only for that node\n",
    "- Other nodes CANNOT see it\n",
    "\n",
    "**When to use**:\n",
    "- Tracking invocation counts\n",
    "- Caching computed values\n",
    "- Retry attempts or internal state\n",
    "- Node-specific configuration\n",
    "\n",
    "```python\n",
    "async def process(self, context):\n",
    "    # Track this node's own invocation count\n",
    "    if 'count' not in context.state:\n",
    "        context.state['count'] = 0\n",
    "    context.state['count'] += 1\n",
    "    \n",
    "    # Use it for node-specific logic\n",
    "    if context.state['count'] > 5:\n",
    "        print(\"This node has been called 5+ times\")\n",
    "    \n",
    "    return {'invocations': context.state['count']}\n",
    "```\n",
    "\n",
    "#### 3. Graph State - Globally Shared Data\n",
    "\n",
    "**Purpose**: Store data that ALL nodes in the graph need to access or modify\n",
    "\n",
    "**How it works**:\n",
    "- Accessed via `context.graph_state` (async methods)\n",
    "- Shared across ALL nodes in the graph\n",
    "- Thread-safe with automatic locking in concurrent mode\n",
    "- Persists across multiple `graph.run()` calls\n",
    "\n",
    "**When to use**:\n",
    "- Shared counters (total items processed across all nodes)\n",
    "- Accumulated results (collecting outputs from multiple branches)\n",
    "- Coordination flags (should_stop, is_ready, etc.)\n",
    "- Global configuration that all nodes need\n",
    "\n",
    "```python\n",
    "async def process(self, context):\n",
    "    # Read shared counter\n",
    "    counter = await context.graph_state.get('total_items', 0)\n",
    "    \n",
    "    # Update it\n",
    "    await context.graph_state.set('total_items', counter + 1)\n",
    "    \n",
    "    # Append to shared list\n",
    "    results = await context.graph_state.get('all_results', [])\n",
    "    results.append(my_result)\n",
    "    await context.graph_state.set('all_results', results)\n",
    "    \n",
    "    # Or use transaction for atomic updates\n",
    "    async with context.graph_state.transaction() as state:\n",
    "        state['x'] = state.get('x', 0) + 1\n",
    "        state['y'] = state.get('y', 0) + 1\n",
    "    \n",
    "    return {'done': True}\n",
    "```\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Feature | Outputs | Node State | Graph State |\n",
    "|---------|---------|------------|-------------|\n",
    "| **Scope** | Next node(s) only | Single node only | All nodes |\n",
    "| **Lifetime** | One execution | Across invocations | Across invocations & runs |\n",
    "| **Access** | `context.inputs.content` | `context.state` | `context.graph_state` |\n",
    "| **Flow** | Follows graph edges | Doesn't flow | Available everywhere |\n",
    "| **Thread-safe** | N/A | Yes | Yes (auto-locking) |\n",
    "| **Use for** | Transformed data | Node-specific state | Coordination & sharing |\n",
    "\n",
    "### Decision Tree: Which Storage Type to Use?\n",
    "\n",
    "```\n",
    "Do multiple non-adjacent nodes need this data?\n",
    "â”œâ”€ YES â†’ Use Graph State\n",
    "â””â”€ NO\n",
    "   â””â”€ Does the next node in the pipeline need this data?\n",
    "      â”œâ”€ YES â†’ Use Outputs\n",
    "      â””â”€ NO\n",
    "         â””â”€ Does only THIS node need it across invocations?\n",
    "            â”œâ”€ YES â†’ Use Node State\n",
    "            â””â”€ NO â†’ Don't store it\n",
    "```\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "#### Pattern 1: Pipeline Transformation (Use Outputs)\n",
    "```python\n",
    "# Each node transforms and passes data forward\n",
    "input_node >> processor >> enricher >> output_node\n",
    "```\n",
    "\n",
    "#### Pattern 2: Shared Counter (Use Graph State)\n",
    "```python\n",
    "# Multiple nodes increment a shared counter\n",
    "async def process(self, context):\n",
    "    counter = await context.graph_state.get('counter', 0)\n",
    "    await context.graph_state.set('counter', counter + 1)\n",
    "    return {'done': True}\n",
    "```\n",
    "\n",
    "#### Pattern 3: Node Invocation Tracking (Use Node State)\n",
    "```python\n",
    "# Track how many times THIS node has run\n",
    "async def process(self, context):\n",
    "    context.state['runs'] = context.state.get('runs', 0) + 1\n",
    "    return {'run_count': context.state['runs']}\n",
    "```\n",
    "\n",
    "### Graph State Initialization and Access\n",
    "\n",
    "```python\n",
    "# Initialize graph with state\n",
    "graph = Graph(\n",
    "    start=start_node,\n",
    "    initial_state={'counter': 0, 'results': [], 'config': {...}}\n",
    ")\n",
    "\n",
    "# Run the graph\n",
    "result = await graph.run()\n",
    "\n",
    "# Access state after execution\n",
    "counter = await graph.get_state('counter')\n",
    "all_state = graph.get_state_snapshot()\n",
    "\n",
    "# Reset for next run\n",
    "graph.reset_state({'counter': 0})\n",
    "```\n",
    "\n",
    "### ExecutionContext Structure\n",
    "\n",
    "```python\n",
    "class ExecutionContext:\n",
    "    inputs: NodeMessage           # Data from previous node (FLOWS via outputs)\n",
    "    state: NodeState              # This node's own state (PRIVATE, persists)\n",
    "    graph_state: GraphState       # Shared state (GLOBAL, accessible to all)\n",
    "    metadata: ExecutionMetadata   # Execution timing/attempts\n",
    "    outputs: Any                  # Current node's output (becomes next inputs)\n",
    "```\n",
    "\n",
    "**Remember**:\n",
    "- `context.inputs` = data from parent node's outputs (flows one direction)\n",
    "- `context.state` = this node's private state (stays local, persists)\n",
    "- `context.graph_state` = shared across all nodes (global, persists)\n",
    "- Return value = outputs that flow to successor nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Create a 3-Node Calculator Pipeline\n",
    "\n",
    "Create a pipeline that:\n",
    "1. Takes two numbers as input\n",
    "2. Calculates sum, difference, and product\n",
    "3. Formats the results as a string\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "class NumberInput(Node):\n",
    "    async def process(self, context):\n",
    "        # Implement this\n",
    "        pass\n",
    "\n",
    "class Calculator(Node):\n",
    "    async def process(self, context):\n",
    "        # Implement this\n",
    "        pass\n",
    "\n",
    "class Formatter(Node):\n",
    "    async def process(self, context):\n",
    "        # Implement this\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "class NumberInput(Node):\n",
    "    \"\"\"Inputs two numbers for calculation.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        inputs = context.inputs.content\n",
    "        num1 = inputs.get('num1', 5)\n",
    "        num2 = inputs.get('num2', 3)\n",
    "        \n",
    "        print(f\"ðŸ”¢ Input numbers: {num1} and {num2}\")\n",
    "        \n",
    "        return {\n",
    "            'number1': num1,\n",
    "            'number2': num2,\n",
    "            'operation': 'basic_arithmetic'\n",
    "        }\n",
    "\n",
    "class Calculator(Node):\n",
    "    \"\"\"Performs calculations on two numbers.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        num1 = data.get('number1', 0)\n",
    "        num2 = data.get('number2', 0)\n",
    "        \n",
    "        print(f\"ðŸ§® Calculating operations for {num1} and {num2}...\")\n",
    "        \n",
    "        calculations = {\n",
    "            'sum': num1 + num2,\n",
    "            'difference': num1 - num2,\n",
    "            'product': num1 * num2,\n",
    "            'quotient': num1 / num2 if num2 != 0 else 'undefined',\n",
    "            'power': num1 ** num2\n",
    "        }\n",
    "        \n",
    "        result = data.copy()\n",
    "        result['calculations'] = calculations\n",
    "        \n",
    "        return result\n",
    "\n",
    "class Formatter(Node):\n",
    "    \"\"\"Formats calculation results.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        num1 = data.get('number1', 0)\n",
    "        num2 = data.get('number2', 0)\n",
    "        calc = data.get('calculations', {})\n",
    "        \n",
    "        print(\"ðŸ“ Formatting results...\")\n",
    "        \n",
    "        formatted_results = f\"\"\"\n",
    "=== Calculation Results ===\n",
    "Numbers: {num1} and {num2}\n",
    "Sum: {num1} + {num2} = {calc.get('sum', 'N/A')}\n",
    "Difference: {num1} - {num2} = {calc.get('difference', 'N/A')}\n",
    "Product: {num1} Ã— {num2} = {calc.get('product', 'N/A')}\n",
    "Quotient: {num1} Ã· {num2} = {calc.get('quotient', 'N/A')}\n",
    "Power: {num1}^{num2} = {calc.get('power', 'N/A')}\n",
    "============================\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        print(formatted_results)\n",
    "        \n",
    "        return {\n",
    "            'formatted_output': formatted_results,\n",
    "            'calculation_complete': True\n",
    "        }\n",
    "\n",
    "# Create and test the calculator pipeline\n",
    "input_node = NumberInput()\n",
    "calc_node = Calculator()\n",
    "format_node = Formatter()\n",
    "\n",
    "# Connect nodes\n",
    "input_node >> calc_node >> format_node\n",
    "\n",
    "# Create graph\n",
    "calculator_graph = Graph(start=input_node)\n",
    "\n",
    "print(\"=== Exercise 1: Calculator Pipeline ===\")\n",
    "result = await calculator_graph.run({'num1': 12, 'num2': 4})\n",
    "\n",
    "print(f\"\\nâœ… Calculator pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Text Processing Workflow\n",
    "\n",
    "Create a workflow that processes text through multiple steps:\n",
    "1. Input text\n",
    "2. Count words and characters\n",
    "3. Find the longest word\n",
    "4. Generate a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "class TextInput(Node):\n",
    "    \"\"\"Inputs text for processing.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        text = context.inputs.content.get('text', 'The quick brown fox jumps over the lazy dog')\n",
    "        \n",
    "        print(f\"ðŸ“ Input text: '{text}'\")\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'processing_step': 'input_complete'\n",
    "        }\n",
    "\n",
    "class TextCounter(Node):\n",
    "    \"\"\"Counts words and characters.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        text = data.get('original_text', '')\n",
    "        \n",
    "        print(\"ðŸ”¢ Counting words and characters...\")\n",
    "        \n",
    "        words = text.split()\n",
    "        characters = len(text)\n",
    "        characters_no_spaces = len(text.replace(' ', ''))\n",
    "        \n",
    "        result = data.copy()\n",
    "        result.update({\n",
    "            'word_count': len(words),\n",
    "            'character_count': characters,\n",
    "            'character_count_no_spaces': characters_no_spaces,\n",
    "            'words': words\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "class WordAnalyzer(Node):\n",
    "    \"\"\"Finds longest word and analyzes vocabulary.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        words = data.get('words', [])\n",
    "        \n",
    "        print(\"ðŸ” Analyzing words...\")\n",
    "        \n",
    "        if words:\n",
    "            longest_word = max(words, key=len)\n",
    "            shortest_word = min(words, key=len)\n",
    "            average_word_length = sum(len(word) for word in words) / len(words)\n",
    "            \n",
    "            # Count unique words\n",
    "            unique_words = set(word.lower() for word in words)\n",
    "        else:\n",
    "            longest_word = shortest_word = ''\n",
    "            average_word_length = 0\n",
    "            unique_words = set()\n",
    "        \n",
    "        result = data.copy()\n",
    "        result.update({\n",
    "            'longest_word': longest_word,\n",
    "            'shortest_word': shortest_word,\n",
    "            'average_word_length': round(average_word_length, 2),\n",
    "            'unique_word_count': len(unique_words),\n",
    "            'vocabulary': list(unique_words)\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "class SummaryGenerator(Node):\n",
    "    \"\"\"Generates a final summary.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        data = context.inputs.content\n",
    "        \n",
    "        print(\"ðŸ“Š Generating summary...\")\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "=== Text Analysis Summary ===\n",
    "Original Text: {data.get('original_text', 'N/A')}\n",
    "Words: {data.get('word_count', 0)}\n",
    "Characters (with spaces): {data.get('character_count', 0)}\n",
    "Characters (no spaces): {data.get('character_count_no_spaces', 0)}\n",
    "Longest Word: {data.get('longest_word', 'N/A')}\n",
    "Shortest Word: {data.get('shortest_word', 'N/A')}\n",
    "Average Word Length: {data.get('average_word_length', 0)}\n",
    "Unique Words: {data.get('unique_word_count', 0)}\n",
    "============================\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        print(summary)\n",
    "        \n",
    "        return {\n",
    "            'summary': summary,\n",
    "            'analysis_complete': True,\n",
    "            'processing_steps': ['input', 'counting', 'analysis', 'summary']\n",
    "        }\n",
    "\n",
    "# Create text processing pipeline\n",
    "text_input = TextInput()\n",
    "text_counter = TextCounter()\n",
    "word_analyzer = WordAnalyzer()\n",
    "summary_gen = SummaryGenerator()\n",
    "\n",
    "# Connect nodes\n",
    "text_input >> text_counter >> word_analyzer >> summary_gen\n",
    "\n",
    "# Create graph\n",
    "text_pipeline = Graph(start=text_input)\n",
    "\n",
    "print(\"=== Exercise 2: Text Processing Workflow ===\")\n",
    "test_text = \"Spark makes building AI workflows easy and powerful\"\n",
    "result = await text_pipeline.run({'text': test_text})\n",
    "\n",
    "print(f\"\\nâœ… Text processing completed!\")\n",
    "print(f\"Processing steps: {result.content['processing_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Experiment with Data Transformations\n",
    "\n",
    "Create your own pipeline that transforms data in interesting ways. Be creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create Your Own Pipeline\n",
    "# Example: Number Transformation Pipeline\n",
    "\n",
    "class NumberGenerator(Node):\n",
    "    \"\"\"Generates a sequence of numbers.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        start = context.inputs.content.get('start', 1)\n",
    "        end = context.inputs.content.get('end', 10)\n",
    "        \n",
    "        numbers = list(range(start, end + 1))\n",
    "        print(f\"ðŸ”¢ Generated numbers: {numbers}\")\n",
    "        \n",
    "        return {'numbers': numbers, 'range': f\"{start}-{end}\"}\n",
    "\n",
    "class NumberTransformer(Node):\n",
    "    \"\"\"Transforms numbers in various ways.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        numbers = context.inputs.content.get('numbers', [])\n",
    "        \n",
    "        transformations = {\n",
    "            'squares': [n**2 for n in numbers],\n",
    "            'cubes': [n**3 for n in numbers],\n",
    "            'doubles': [n*2 for n in numbers],\n",
    "            'evens': [n for n in numbers if n % 2 == 0],\n",
    "            'odds': [n for n in numbers if n % 2 != 0]\n",
    "        }\n",
    "        \n",
    "        print(\"ðŸ”„ Applied transformations:\")\n",
    "        for name, values in transformations.items():\n",
    "            print(f\"  {name}: {values}\")\n",
    "\n",
    "        return {\n",
    "            'numbers': numbers,\n",
    "            'transformations': transformations\n",
    "        }\n",
    "\n",
    "\n",
    "class StatisticsCalculator(Node):\n",
    "    \"\"\"Calculates statistics on transformed data.\"\"\"\n",
    "    \n",
    "    async def process(self, context):\n",
    "        inputs = context.inputs.content\n",
    "        numbers = inputs.get('numbers', [])\n",
    "        transformations = inputs.get('transformations', {})\n",
    "        \n",
    "        stats = {\n",
    "            'original_sum': sum(numbers),\n",
    "            'original_avg': sum(numbers) / len(numbers) if numbers else 0,\n",
    "            'squares_sum': sum(transformations.get('squares', [])),\n",
    "            'even_count': len(transformations.get('evens', [])),\n",
    "            'odd_count': len(transformations.get('odds', []))\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ“Š Statistics: {stats}\")\n",
    "        \n",
    "        result = dict(inputs)\n",
    "        result['statistics'] = stats\n",
    "        return result\n",
    "\n",
    "# Create your custom pipeline\n",
    "generator = NumberGenerator()\n",
    "transformer = NumberTransformer()\n",
    "stats_calc = StatisticsCalculator()\n",
    "\n",
    "# Connect your nodes\n",
    "generator >> transformer >> stats_calc\n",
    "\n",
    "# Create graph\n",
    "custom_pipeline = Graph(start=generator)\n",
    "\n",
    "print(\"=== Exercise 3: Custom Number Transformation Pipeline ===\")\n",
    "result = await custom_pipeline.run({'start': 3, 'end': 8})\n",
    "\n",
    "print(f\"\\nâœ… Custom pipeline completed!\")\n",
    "print(f\"Final statistics: {result.content['statistics']}\")\n",
    "\n",
    "# Now try modifying this pipeline or creating your own!\n",
    "print(\"\\nðŸ’¡ Try creating your own pipeline with different transformations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- âœ… **Graph Fundamentals**: How to create and connect nodes into flows\n",
    "- âœ… **Edge Creation**: Using the `>>` operator to connect nodes\n",
    "- âœ… **Auto-Discovery**: How Spark automatically discovers connected nodes\n",
    "- âœ… **ExecutionContext**: Understanding how data flows between nodes\n",
    "- âœ… **Data Passing**: Using `context.inputs` and return values\n",
    "- âœ… **Three Storage Types**: CRITICAL distinction between outputs, node state, and graph state\n",
    "- âœ… **Graph State**: Using globally shared state for coordination across nodes\n",
    "- âœ… **Pipeline Building**: Creating multi-step data processing workflows\n",
    "\n",
    "### Key Concepts Mastered\n",
    "\n",
    "- Node creation and the `process()` method\n",
    "- Graph construction with auto-discovery\n",
    "- Data flow patterns in Spark\n",
    "- ExecutionContext structure and usage\n",
    "- **The Actor Model: Three distinct storage mechanisms**\n",
    "  - Outputs for sequential data flow\n",
    "  - Node state for private persistence\n",
    "  - Graph state for global coordination\n",
    "- Building practical data pipelines\n",
    "- Error handling in node processing\n",
    "- Correct state usage for different scenarios\n",
    "\n",
    "### The Most Important Lesson\n",
    "\n",
    "**Three Storage Types - Choose the Right One!**\n",
    "\n",
    "```python\n",
    "# âŒ WRONG: Node state doesn't flow between nodes\n",
    "context.state['for_next_node'] = data\n",
    "\n",
    "# âœ… RIGHT: Outputs flow to next nodes\n",
    "return {'for_next_node': data}\n",
    "\n",
    "# âœ… RIGHT: Graph state is accessible everywhere\n",
    "await context.graph_state.set('shared_counter', 42)\n",
    "\n",
    "# âœ… RIGHT: Use node state for node's own data only\n",
    "context.state['my_invocation_count'] += 1\n",
    "```\n",
    "\n",
    "### Decision Guide: Which Storage Type?\n",
    "\n",
    "**Use Outputs when:**\n",
    "- Data flows linearly through the pipeline\n",
    "- Each node transforms the data for the next node\n",
    "- The graph structure defines the data flow\n",
    "\n",
    "**Use Node State when:**\n",
    "- Tracking a node's own invocation count or internal state\n",
    "- Caching results specific to one node\n",
    "- Data should NOT be visible to other nodes\n",
    "\n",
    "**Use Graph State when:**\n",
    "- Multiple nodes need to coordinate (counters, flags)\n",
    "- Accumulating results from different parts of the graph\n",
    "- Sharing configuration or context across all nodes\n",
    "- Nodes that aren't directly connected need to share data\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Simple data pipeline (outputs)\n",
    "input_node >> processor_node >> output_node\n",
    "graph = Graph(start_node=input_node)\n",
    "\n",
    "# Pattern 2: Shared counter (graph state)\n",
    "graph = Graph(start_node=start_node, initial_state={'counter': 0})\n",
    "# In nodes: await context.graph_state.set('counter', counter + 1)\n",
    "\n",
    "# Pattern 3: Node-specific tracking (node state)\n",
    "# In node: context.state['invocations'] = context.state.get('invocations', 0) + 1\n",
    "\n",
    "# Pattern 4: Data validation and enrichment (outputs)\n",
    "validator >> enricher >> aggregator >> exporter\n",
    "\n",
    "# Pattern 5: Multi-step transformation (outputs + graph state)\n",
    "collector >> cleaner >> analyzer >> reporter\n",
    "# Use outputs for transformed data, graph state for metrics\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Single Responsibility**: Each node should do one thing well\n",
    "2. **Clear Data Flow**: Make it obvious what data each node expects and returns\n",
    "3. **Use Outputs for Sequential Flow**: When data goes node â†’ next node\n",
    "4. **Use Graph State for Coordination**: When multiple nodes need shared data\n",
    "5. **Use Node State for Privacy**: When only that node needs the data\n",
    "6. **Error Handling**: Always validate inputs in your nodes\n",
    "7. **Descriptive Names**: Use clear node and variable names\n",
    "8. **Initialize Graph State**: Set initial values when creating the graph\n",
    "\n",
    "### Graph State Examples\n",
    "\n",
    "```python\n",
    "# Initialize with state\n",
    "graph = Graph(\n",
    "    start=start_node,\n",
    "    initial_state={'counter': 0, 'results': [], 'is_ready': False}\n",
    ")\n",
    "\n",
    "# Run and access state\n",
    "result = await graph.run()\n",
    "counter = await graph.get_state('counter')\n",
    "\n",
    "# Reset between runs\n",
    "graph.reset_state({'counter': 0, 'results': []})\n",
    "```\n",
    "\n",
    "### Next Tutorial\n",
    "\n",
    "In **Tutorial 4: Your First AI Agent**, you'll learn how to:\n",
    "- Understand the difference between Nodes and Agents\n",
    "- Create and configure an Agent\n",
    "- Work with different LLM providers\n",
    "- Handle agent inputs and outputs\n",
    "- Use AgentConfig for agent configuration\n",
    "\n",
    "### Continue Your Learning\n",
    "\n",
    "- Try building pipelines for your own data processing needs\n",
    "- Experiment with graph state for coordination patterns\n",
    "- Practice with the example file: `e006_graph_state_counter.py`\n",
    "- Think about when to use outputs vs. graph state\n",
    "- Remember: **Choose the right storage type for the job!**\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ‰ **Congratulations!** You've mastered the fundamentals of creating flows and graphs in Spark, including the critical distinction between the three storage types: outputs for sequential flow, node state for private data, and graph state for global coordination. This Actor Model foundation with proper isolation and sharing patterns is essential as we move on to working with AI agents in the next tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
